{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, random, shutil\n",
    "from scipy.io import wavfile\n",
    "from shutil import copyfile\n",
    "from sphfile import SPHFile\n",
    "from WaveNetClassifier import WaveNetClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each person has 10 audio samples.\n",
    "\n",
    "# Normalize the data before saving them to arrays.\n",
    "nb_bits = 16\n",
    "max_nb_bit = float(2 ** (nb_bits - 1))\n",
    "\n",
    "def process_audio(base_dir, audio_data, user_ids, test_audio_data, test_user_ids, no_users):\n",
    "    min_files = 0\n",
    "    for i, user_id in enumerate(os.listdir(base_dir)):\n",
    "        if i == no_users:\n",
    "            return\n",
    "        current_dir = os.path.join(base_dir, user_id)\n",
    "        for i, audio_file in enumerate(os.listdir(current_dir)):\n",
    "            no_samples = len(os.listdir(current_dir))\n",
    "            file_path = os.path.join(current_dir, audio_file)\n",
    "            fs, data = wavfile.read(file_path)\n",
    "            if i < no_samples - 1:\n",
    "                user_ids.append(user_id)\n",
    "                audio_data.append(data / (max_nb_bit + 1.0))\n",
    "            else:\n",
    "                test_user_ids.append(user_id)\n",
    "                test_audio_data.append(data / (max_nb_bit + 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_1_sample(base_dir, audio_data, user_ids, test_audio_data, test_user_ids, no_users):\n",
    "    min_files = 0\n",
    "    for i, user_id in enumerate(os.listdir(base_dir)):\n",
    "        if i == no_users:\n",
    "            return\n",
    "        current_dir = os.path.join(base_dir, user_id)\n",
    "        for i, audio_file in enumerate(os.listdir(current_dir)):\n",
    "            file_path = os.path.join(current_dir, audio_file)\n",
    "            fs, data = wavfile.read(file_path)\n",
    "            if i == 1:\n",
    "                user_ids.append(user_id)\n",
    "                audio_data.append(data / (max_nb_bit + 1.0))\n",
    "            elif i == 2:\n",
    "                test_user_ids.append(user_id)\n",
    "                test_audio_data.append(data / (max_nb_bit + 1.0))\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_2_samples(base_dir, audio_data, user_ids, test_audio_data, test_user_ids, no_users):\n",
    "    min_files = 0\n",
    "    for i, user_id in enumerate(os.listdir(base_dir)):\n",
    "        if i == no_users:\n",
    "            return\n",
    "        current_dir = os.path.join(base_dir, user_id)\n",
    "        for i, audio_file in enumerate(os.listdir(current_dir)):\n",
    "            file_path = os.path.join(current_dir, audio_file)\n",
    "            fs, data = wavfile.read(file_path)\n",
    "            if i < 2:\n",
    "                user_ids.append(user_id)\n",
    "                audio_data.append(data / (max_nb_bit + 1.0))\n",
    "            elif i == 2:\n",
    "                test_user_ids.append(user_id)\n",
    "                test_audio_data.append(data / (max_nb_bit + 1.0))\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_audio(audio_data, test_audio_data):\n",
    "    min_length_audio = min(audio_data, key = len)\n",
    "    min_length_test_audio = min(test_audio_data, key = len)\n",
    "    min_length = min(len(min_length_audio), len(min_length_test_audio))\n",
    "    for i, sample in enumerate(audio_data):\n",
    "        audio_data[i] = sample[0 : min_length]\n",
    "    for i, sample in enumerate(test_audio_data):\n",
    "        test_audio_data[i] = sample[0 : min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove samples less than x length.\n",
    "\n",
    "base_dir = 'TRAIN_WAV_NO_DR_64k'\n",
    "\n",
    "for user_id in os.listdir(base_dir):\n",
    "    current_dir = os.path.join(base_dir, user_id)\n",
    "    for audio_file in os.listdir(current_dir):\n",
    "        file_path = os.path.join(current_dir, audio_file)\n",
    "        fs, data = wavfile.read(file_path)\n",
    "        if len(data) < 64000:\n",
    "            if(os.path.exists(file_path)):\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Delete users with 0 or 1 samples.\n",
    "\n",
    "base_dir = 'TRAIN_WAV_NO_DR'\n",
    "\n",
    "for user_id in os.listdir(base_dir):\n",
    "    current_dir = os.path.join(base_dir, user_id)\n",
    "    if len(os.listdir(current_dir)) < 2:\n",
    "        shutil.rmtree(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List no of samples for each user.\n",
    "\n",
    "base_dir = 'TRAIN_WAV_NO_DR_48k'\n",
    "\n",
    "lens = []\n",
    "\n",
    "for user_id in os.listdir(base_dir):\n",
    "    current_dir = os.path.join(base_dir, user_id)\n",
    "    lens.append(len(os.listdir(current_dir)))\n",
    "                \n",
    "sorted(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_data = []\n",
    "user_ids = []\n",
    "test_audio_data = []\n",
    "test_user_ids = []\n",
    "base_dir = 'TRAIN_WAV_NO_DR'\n",
    "no_users = 6\n",
    "max_no_users = len(os.listdir(base_dir))\n",
    "\n",
    "process_audio(base_dir, audio_data, user_ids, test_audio_data, test_user_ids, 100)\n",
    "\n",
    "display(len(audio_data))\n",
    "display(len(user_ids))\n",
    "display(len(test_audio_data))\n",
    "display(len(test_user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18330"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18330"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unify_audio(audio_data, test_audio_data)\n",
    "\n",
    "display(len(audio_data[4]))\n",
    "display(len(test_audio_data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(900, 18330)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train shape:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(900, 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a vector for each user id. Categorical -> one hot encoding\n",
    "# The list has to be converted the numpy array and transposed first.\n",
    "\n",
    "X_train = np.array(audio_data)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse = False, categories = 'auto')\n",
    "Y_train = onehot_encoder.fit_transform(np.array(user_ids).reshape(-1, 1))\n",
    "\n",
    "print('X_train shape:')\n",
    "display(X_train.shape)\n",
    "\n",
    "print('Y_train shape:')\n",
    "display(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 32052)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 32052, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 32052, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 32052, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 32052, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 32052, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 32052, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 32052, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 32052, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 32052, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 32052, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 32052, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 32052, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 32052, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 32052, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 32052, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 32052, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 32052, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 32052, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 32052, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 32052, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 32052, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 32052, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 32052, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 32052, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 32052, 40)    0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 32052, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 32052, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 32052, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 32052, 40)    0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 32052, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 32052, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 32052, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 32052, 40)    0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 32052, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_tanh (Conv1D)  (None, 32052, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_sigm (Conv1D)  (None, 32052, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 32052, 40)    0           dilated_conv_512_tanh[0][0]      \n",
      "                                                                 dilated_conv_512_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 32052, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 32052, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32052, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 32052, 40)    128040      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 401, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 401, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 401, 6)       24006       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 5, 6)         0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 5, 6)         150         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 6)         0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 6)            0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 6)            0           reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 385,436\n",
      "Trainable params: 385,436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wnc = WaveNetClassifier((32052,), (6,), kernel_size = 2, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "51/51 [==============================] - 11s 223ms/step - loss: 1.8128 - acc: 0.1373\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.81276, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7931 - acc: 0.1765\n",
      "\n",
      "Epoch 00002: loss improved from 1.81276 to 1.79309, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 1.7918 - acc: 0.1961\n",
      "\n",
      "Epoch 00003: loss improved from 1.79309 to 1.79179, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 4/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7918 - acc: 0.1765\n",
      "\n",
      "Epoch 00004: loss did not improve from 1.79179\n",
      "Epoch 5/200\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 1.7918 - acc: 0.1569\n",
      "\n",
      "Epoch 00005: loss improved from 1.79179 to 1.79178, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7917 - acc: 0.1569\n",
      "\n",
      "Epoch 00006: loss improved from 1.79178 to 1.79170, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 7/200\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 1.7916 - acc: 0.1569\n",
      "\n",
      "Epoch 00007: loss improved from 1.79170 to 1.79164, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 1.7918 - acc: 0.1569\n",
      "\n",
      "Epoch 00008: loss did not improve from 1.79164\n",
      "Epoch 9/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7917 - acc: 0.1569\n",
      "\n",
      "Epoch 00009: loss did not improve from 1.79164\n",
      "Epoch 10/200\n",
      "51/51 [==============================] - 5s 95ms/step - loss: 1.7916 - acc: 0.1569\n",
      "\n",
      "Epoch 00010: loss improved from 1.79164 to 1.79164, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 11/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7915 - acc: 0.1569\n",
      "\n",
      "Epoch 00011: loss improved from 1.79164 to 1.79152, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 12/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7915 - acc: 0.1569\n",
      "\n",
      "Epoch 00012: loss improved from 1.79152 to 1.79146, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 13/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7915 - acc: 0.2549\n",
      "\n",
      "Epoch 00013: loss did not improve from 1.79146\n",
      "Epoch 14/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7915 - acc: 0.1765\n",
      "\n",
      "Epoch 00014: loss did not improve from 1.79146\n",
      "Epoch 15/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7914 - acc: 0.1765\n",
      "\n",
      "Epoch 00015: loss improved from 1.79146 to 1.79141, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 16/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7914 - acc: 0.1765\n",
      "\n",
      "Epoch 00016: loss did not improve from 1.79141\n",
      "Epoch 17/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7913 - acc: 0.1765\n",
      "\n",
      "Epoch 00017: loss improved from 1.79141 to 1.79135, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 18/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7914 - acc: 0.1765\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.79135\n",
      "Epoch 19/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7913 - acc: 0.1765\n",
      "\n",
      "Epoch 00019: loss improved from 1.79135 to 1.79132, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 20/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7913 - acc: 0.1765\n",
      "\n",
      "Epoch 00020: loss improved from 1.79132 to 1.79131, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 21/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7913 - acc: 0.1765\n",
      "\n",
      "Epoch 00021: loss improved from 1.79131 to 1.79127, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 22/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7912 - acc: 0.1765\n",
      "\n",
      "Epoch 00022: loss improved from 1.79127 to 1.79123, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 23/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7912 - acc: 0.1765\n",
      "\n",
      "Epoch 00023: loss improved from 1.79123 to 1.79123, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7912 - acc: 0.1765\n",
      "\n",
      "Epoch 00024: loss improved from 1.79123 to 1.79122, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 25/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7911 - acc: 0.1765\n",
      "\n",
      "Epoch 00025: loss improved from 1.79122 to 1.79114, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 26/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7912 - acc: 0.1765\n",
      "\n",
      "Epoch 00026: loss did not improve from 1.79114\n",
      "Epoch 27/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7911 - acc: 0.1765\n",
      "\n",
      "Epoch 00027: loss improved from 1.79114 to 1.79109, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 28/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7912 - acc: 0.1765\n",
      "\n",
      "Epoch 00028: loss did not improve from 1.79109\n",
      "Epoch 29/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7910 - acc: 0.1765\n",
      "\n",
      "Epoch 00029: loss improved from 1.79109 to 1.79102, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 30/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7910 - acc: 0.1765\n",
      "\n",
      "Epoch 00030: loss improved from 1.79102 to 1.79101, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 31/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7910 - acc: 0.1765\n",
      "\n",
      "Epoch 00031: loss improved from 1.79101 to 1.79099, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 32/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7910 - acc: 0.1765\n",
      "\n",
      "Epoch 00032: loss improved from 1.79099 to 1.79097, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 33/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7909 - acc: 0.1765\n",
      "\n",
      "Epoch 00033: loss improved from 1.79097 to 1.79094, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 34/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7909 - acc: 0.1765\n",
      "\n",
      "Epoch 00034: loss did not improve from 1.79094\n",
      "Epoch 35/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7909 - acc: 0.1765\n",
      "\n",
      "Epoch 00035: loss improved from 1.79094 to 1.79088, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 36/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7908 - acc: 0.1765\n",
      "\n",
      "Epoch 00036: loss improved from 1.79088 to 1.79084, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 37/200\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 1.7908 - acc: 0.1765\n",
      "\n",
      "Epoch 00037: loss improved from 1.79084 to 1.79084, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 38/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7909 - acc: 0.1765\n",
      "\n",
      "Epoch 00038: loss did not improve from 1.79084\n",
      "Epoch 39/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7909 - acc: 0.1765\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.79084\n",
      "Epoch 40/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7909 - acc: 0.1765\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.79084\n",
      "Epoch 41/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7908 - acc: 0.1765\n",
      "\n",
      "Epoch 00041: loss improved from 1.79084 to 1.79084, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 42/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7909 - acc: 0.1765\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.79084\n",
      "Epoch 43/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7908 - acc: 0.1765\n",
      "\n",
      "Epoch 00043: loss improved from 1.79084 to 1.79078, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 44/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7908 - acc: 0.1765\n",
      "\n",
      "Epoch 00044: loss did not improve from 1.79078\n",
      "Epoch 45/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7908 - acc: 0.1765\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.79078\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 5s 98ms/step - loss: 1.7907 - acc: 0.1765\n",
      "\n",
      "Epoch 00046: loss improved from 1.79078 to 1.79070, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 47/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7908 - acc: 0.1765\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.79070\n",
      "Epoch 48/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7908 - acc: 0.1765\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.79070\n",
      "Epoch 49/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00049: loss improved from 1.79070 to 1.79065, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 50/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00050: loss improved from 1.79065 to 1.79056, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 51/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00051: loss did not improve from 1.79056\n",
      "Epoch 52/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00052: loss did not improve from 1.79056\n",
      "Epoch 53/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00053: loss did not improve from 1.79056\n",
      "Epoch 54/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00054: loss did not improve from 1.79056\n",
      "Epoch 55/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.79056\n",
      "Epoch 56/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7907 - acc: 0.1765\n",
      "\n",
      "Epoch 00056: loss did not improve from 1.79056\n",
      "Epoch 57/200\n",
      "51/51 [==============================] - 5s 99ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00057: loss did not improve from 1.79056\n",
      "Epoch 58/200\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 1.7907 - acc: 0.1765\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.79056\n",
      "Epoch 59/200\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 1.7905 - acc: 0.1765\n",
      "\n",
      "Epoch 00059: loss improved from 1.79056 to 1.79055, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 60/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.79055\n",
      "Epoch 61/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.79055\n",
      "Epoch 62/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00062: loss did not improve from 1.79055\n",
      "Epoch 63/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00063: loss did not improve from 1.79055\n",
      "Epoch 64/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00064: loss did not improve from 1.79055\n",
      "Epoch 65/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7906 - acc: 0.1961\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.79055\n",
      "Epoch 66/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00066: loss did not improve from 1.79055\n",
      "Epoch 67/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7906 - acc: 0.1765\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.79055\n",
      "Epoch 68/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7905 - acc: 0.1765\n",
      "\n",
      "Epoch 00068: loss improved from 1.79055 to 1.79049, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 69/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7905 - acc: 0.1765\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.79049\n",
      "Epoch 70/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7905 - acc: 0.1765\n",
      "\n",
      "Epoch 00070: loss improved from 1.79049 to 1.79047, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 71/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00071: loss improved from 1.79047 to 1.79040, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 72/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7905 - acc: 0.1765\n",
      "\n",
      "Epoch 00072: loss did not improve from 1.79040\n",
      "Epoch 73/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00073: loss did not improve from 1.79040\n",
      "Epoch 74/200\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00074: loss improved from 1.79040 to 1.79039, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 75/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00075: loss improved from 1.79039 to 1.79038, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 76/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00076: loss improved from 1.79038 to 1.79035, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 77/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00077: loss improved from 1.79035 to 1.79033, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 78/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.79033\n",
      "Epoch 79/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00079: loss did not improve from 1.79033\n",
      "Epoch 80/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.79033\n",
      "Epoch 81/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.79033\n",
      "Epoch 82/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00082: loss did not improve from 1.79033\n",
      "Epoch 83/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.79033\n",
      "Epoch 84/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00084: loss improved from 1.79033 to 1.79029, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 85/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7905 - acc: 0.1765\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.79029\n",
      "Epoch 86/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.79029\n",
      "Epoch 87/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00087: loss did not improve from 1.79029\n",
      "Epoch 88/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.79029\n",
      "Epoch 89/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00089: loss did not improve from 1.79029\n",
      "Epoch 90/200\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.79029\n",
      "Epoch 91/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1569\n",
      "\n",
      "Epoch 00091: loss did not improve from 1.79029\n",
      "Epoch 92/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00092: loss did not improve from 1.79029\n",
      "Epoch 93/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7904 - acc: 0.1765\n",
      "\n",
      "Epoch 00093: loss did not improve from 1.79029\n",
      "Epoch 94/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00094: loss improved from 1.79029 to 1.79027, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 95/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.79027\n",
      "Epoch 96/200\n",
      "51/51 [==============================] - 5s 98ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00096: loss improved from 1.79027 to 1.79026, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00097: loss improved from 1.79026 to 1.79026, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 98/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.79026\n",
      "Epoch 99/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.79026\n",
      "Epoch 100/200\n",
      "51/51 [==============================] - 5s 97ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00100: loss improved from 1.79026 to 1.79026, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 101/200\n",
      "51/51 [==============================] - 5s 96ms/step - loss: 1.7903 - acc: 0.1765\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.79026\n",
      "Epoch 102/200\n",
      "20/51 [==========>...................] - ETA: 2s - loss: 1.7936 - acc: 0.2000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-e0e8e8afa206>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 10, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 48333)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 48333, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 48333, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 48333, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 48333, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 48333, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 48333, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 48333, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 48333, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 48333, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 48333, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 48333, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 48333, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 48333, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 48333, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 48333, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 48333, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 48333, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 48333, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 48333, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 48333, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 48333, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 48333, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 48333, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 48333, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 48333, 40)    0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 48333, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 48333, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 48333, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 48333, 40)    0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 48333, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 48333, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 48333, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 48333, 40)    0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 48333, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_tanh (Conv1D)  (None, 48333, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_sigm (Conv1D)  (None, 48333, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 48333, 40)    0           dilated_conv_512_tanh[0][0]      \n",
      "                                                                 dilated_conv_512_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 48333, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 48333, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48333, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 48333, 40)    128040      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 605, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 605, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 605, 6)       24006       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 7, 6)         0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 7, 6)         222         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 6)         0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 6)            0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 6)            0           reshape_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 385,508\n",
      "Trainable params: 385,508\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wnc = WaveNetClassifier((48333,), (6,), kernel_size = 2, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - 10s 358ms/step - loss: 1.8007 - acc: 0.1071\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.80071, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7915 - acc: 0.2500\n",
      "\n",
      "Epoch 00002: loss improved from 1.80071 to 1.79149, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 1.7911 - acc: 0.2500\n",
      "\n",
      "Epoch 00003: loss improved from 1.79149 to 1.79109, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7908 - acc: 0.2500\n",
      "\n",
      "Epoch 00004: loss improved from 1.79109 to 1.79079, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7905 - acc: 0.2500\n",
      "\n",
      "Epoch 00005: loss improved from 1.79079 to 1.79046, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7901 - acc: 0.2500\n",
      "\n",
      "Epoch 00006: loss improved from 1.79046 to 1.79007, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7898 - acc: 0.2500\n",
      "\n",
      "Epoch 00007: loss improved from 1.79007 to 1.78982, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7895 - acc: 0.2500\n",
      "\n",
      "Epoch 00008: loss improved from 1.78982 to 1.78947, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7892 - acc: 0.2500\n",
      "\n",
      "Epoch 00009: loss improved from 1.78947 to 1.78917, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7888 - acc: 0.2500\n",
      "\n",
      "Epoch 00010: loss improved from 1.78917 to 1.78879, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7886 - acc: 0.2500\n",
      "\n",
      "Epoch 00011: loss improved from 1.78879 to 1.78864, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7884 - acc: 0.2500\n",
      "\n",
      "Epoch 00012: loss improved from 1.78864 to 1.78843, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7880 - acc: 0.2500\n",
      "\n",
      "Epoch 00013: loss improved from 1.78843 to 1.78797, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 1.7877 - acc: 0.2500\n",
      "\n",
      "Epoch 00014: loss improved from 1.78797 to 1.78767, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7873 - acc: 0.2500\n",
      "\n",
      "Epoch 00015: loss improved from 1.78767 to 1.78733, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7872 - acc: 0.2500\n",
      "\n",
      "Epoch 00016: loss improved from 1.78733 to 1.78715, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7868 - acc: 0.2500\n",
      "\n",
      "Epoch 00017: loss improved from 1.78715 to 1.78678, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7864 - acc: 0.2500\n",
      "\n",
      "Epoch 00018: loss improved from 1.78678 to 1.78637, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7862 - acc: 0.2500\n",
      "\n",
      "Epoch 00019: loss improved from 1.78637 to 1.78622, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7860 - acc: 0.2500\n",
      "\n",
      "Epoch 00020: loss improved from 1.78622 to 1.78596, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7856 - acc: 0.2500\n",
      "\n",
      "Epoch 00021: loss improved from 1.78596 to 1.78557, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7853 - acc: 0.2500\n",
      "\n",
      "Epoch 00022: loss improved from 1.78557 to 1.78533, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7851 - acc: 0.2500\n",
      "\n",
      "Epoch 00023: loss improved from 1.78533 to 1.78511, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 1.7848 - acc: 0.2500\n",
      "\n",
      "Epoch 00024: loss improved from 1.78511 to 1.78482, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 1.7845 - acc: 0.2500\n",
      "\n",
      "Epoch 00025: loss improved from 1.78482 to 1.78454, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 1.7842 - acc: 0.2500\n",
      "\n",
      "Epoch 00026: loss improved from 1.78454 to 1.78418, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 1.7841 - acc: 0.2500\n",
      "\n",
      "Epoch 00027: loss improved from 1.78418 to 1.78410, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7837 - acc: 0.2500\n",
      "\n",
      "Epoch 00028: loss improved from 1.78410 to 1.78372, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7836 - acc: 0.2500\n",
      "\n",
      "Epoch 00029: loss improved from 1.78372 to 1.78359, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7832 - acc: 0.2500\n",
      "\n",
      "Epoch 00030: loss improved from 1.78359 to 1.78321, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7829 - acc: 0.2500\n",
      "\n",
      "Epoch 00031: loss improved from 1.78321 to 1.78292, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7826 - acc: 0.2500\n",
      "\n",
      "Epoch 00032: loss improved from 1.78292 to 1.78264, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7824 - acc: 0.2500\n",
      "\n",
      "Epoch 00033: loss improved from 1.78264 to 1.78245, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7824 - acc: 0.2500\n",
      "\n",
      "Epoch 00034: loss improved from 1.78245 to 1.78236, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7819 - acc: 0.2500\n",
      "\n",
      "Epoch 00035: loss improved from 1.78236 to 1.78187, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7817 - acc: 0.2500\n",
      "\n",
      "Epoch 00036: loss improved from 1.78187 to 1.78174, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7815 - acc: 0.2500\n",
      "\n",
      "Epoch 00037: loss improved from 1.78174 to 1.78146, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7813 - acc: 0.2500\n",
      "\n",
      "Epoch 00038: loss improved from 1.78146 to 1.78134, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7809 - acc: 0.2500\n",
      "\n",
      "Epoch 00039: loss improved from 1.78134 to 1.78092, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7809 - acc: 0.2500\n",
      "\n",
      "Epoch 00040: loss improved from 1.78092 to 1.78086, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7806 - acc: 0.2500\n",
      "\n",
      "Epoch 00041: loss improved from 1.78086 to 1.78056, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7804 - acc: 0.2500\n",
      "\n",
      "Epoch 00042: loss improved from 1.78056 to 1.78035, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7802 - acc: 0.2500\n",
      "\n",
      "Epoch 00043: loss improved from 1.78035 to 1.78016, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7799 - acc: 0.2500\n",
      "\n",
      "Epoch 00044: loss improved from 1.78016 to 1.77994, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7796 - acc: 0.2500\n",
      "\n",
      "Epoch 00045: loss improved from 1.77994 to 1.77963, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7794 - acc: 0.2500\n",
      "\n",
      "Epoch 00046: loss improved from 1.77963 to 1.77944, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7792 - acc: 0.2500\n",
      "\n",
      "Epoch 00047: loss improved from 1.77944 to 1.77920, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 1.7790 - acc: 0.2500\n",
      "\n",
      "Epoch 00048: loss improved from 1.77920 to 1.77895, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 1.7789 - acc: 0.2500\n",
      "\n",
      "Epoch 00049: loss improved from 1.77895 to 1.77891, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7786 - acc: 0.2500\n",
      "\n",
      "Epoch 00050: loss improved from 1.77891 to 1.77859, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7784 - acc: 0.2500\n",
      "\n",
      "Epoch 00051: loss improved from 1.77859 to 1.77842, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7782 - acc: 0.2500\n",
      "\n",
      "Epoch 00052: loss improved from 1.77842 to 1.77818, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7781 - acc: 0.2500\n",
      "\n",
      "Epoch 00053: loss improved from 1.77818 to 1.77808, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7778 - acc: 0.2500\n",
      "\n",
      "Epoch 00054: loss improved from 1.77808 to 1.77775, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7775 - acc: 0.2500\n",
      "\n",
      "Epoch 00055: loss improved from 1.77775 to 1.77748, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7773 - acc: 0.2500\n",
      "\n",
      "Epoch 00056: loss improved from 1.77748 to 1.77734, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7773 - acc: 0.2500\n",
      "\n",
      "Epoch 00057: loss improved from 1.77734 to 1.77728, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7770 - acc: 0.2500\n",
      "\n",
      "Epoch 00058: loss improved from 1.77728 to 1.77698, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 1.7768 - acc: 0.2500\n",
      "\n",
      "Epoch 00059: loss improved from 1.77698 to 1.77678, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7766 - acc: 0.2500\n",
      "\n",
      "Epoch 00060: loss improved from 1.77678 to 1.77659, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7764 - acc: 0.2500\n",
      "\n",
      "Epoch 00061: loss improved from 1.77659 to 1.77640, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7762 - acc: 0.2500\n",
      "\n",
      "Epoch 00062: loss improved from 1.77640 to 1.77623, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 1.7760 - acc: 0.2500\n",
      "\n",
      "Epoch 00063: loss improved from 1.77623 to 1.77596, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 1.7759 - acc: 0.2500\n",
      "\n",
      "Epoch 00064: loss improved from 1.77596 to 1.77587, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 1.7757 - acc: 0.2500\n",
      "\n",
      "Epoch 00065: loss improved from 1.77587 to 1.77571, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 1.7755 - acc: 0.2500\n",
      "\n",
      "Epoch 00066: loss improved from 1.77571 to 1.77549, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-b90e54776658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 7, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 67072)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 67072, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 67072, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 67072, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 67072, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 67072, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 67072, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 67072, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 67072, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 67072, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 67072, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 67072, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 67072, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 67072, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 67072, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 67072, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 67072, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 67072, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 67072, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 67072, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 67072, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 67072, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 67072, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 67072, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 67072, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 67072, 40)    0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 67072, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 67072, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 67072, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 67072, 40)    0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 67072, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 67072, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 67072, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 67072, 40)    0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 67072, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_tanh (Conv1D)  (None, 67072, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_sigm (Conv1D)  (None, 67072, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 67072, 40)    0           dilated_conv_512_tanh[0][0]      \n",
      "                                                                 dilated_conv_512_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 67072, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 67072, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 67072, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 67072, 40)    128040      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 839, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 839, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 839, 6)       24006       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 9, 6)         0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 9, 6)         294         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 6)         0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 6)            0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 6)            0           reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 385,580\n",
      "Trainable params: 385,580\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wnc = WaveNetClassifier((67072,), (6,), kernel_size = 2, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 9s 830ms/step - loss: 1.7986 - acc: 0.2727\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.79864, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 1.7763 - acc: 0.2727\n",
      "\n",
      "Epoch 00002: loss improved from 1.79864 to 1.77635, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.8586 - acc: 0.2727\n",
      "\n",
      "Epoch 00003: loss did not improve from 1.77635\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7382 - acc: 0.2727\n",
      "\n",
      "Epoch 00004: loss improved from 1.77635 to 1.73823, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7764 - acc: 0.2727\n",
      "\n",
      "Epoch 00005: loss did not improve from 1.73823\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7734 - acc: 0.2727\n",
      "\n",
      "Epoch 00006: loss did not improve from 1.73823\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7845 - acc: 0.2727\n",
      "\n",
      "Epoch 00007: loss did not improve from 1.73823\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7879 - acc: 0.2727\n",
      "\n",
      "Epoch 00008: loss did not improve from 1.73823\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7886 - acc: 0.2727\n",
      "\n",
      "Epoch 00009: loss did not improve from 1.73823\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7886 - acc: 0.2727\n",
      "\n",
      "Epoch 00010: loss did not improve from 1.73823\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7884 - acc: 0.2727\n",
      "\n",
      "Epoch 00011: loss did not improve from 1.73823\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7881 - acc: 0.2727\n",
      "\n",
      "Epoch 00012: loss did not improve from 1.73823\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7877 - acc: 0.2727\n",
      "\n",
      "Epoch 00013: loss did not improve from 1.73823\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7872 - acc: 0.2727\n",
      "\n",
      "Epoch 00014: loss did not improve from 1.73823\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7867 - acc: 0.2727\n",
      "\n",
      "Epoch 00015: loss did not improve from 1.73823\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7862 - acc: 0.2727\n",
      "\n",
      "Epoch 00016: loss did not improve from 1.73823\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7858 - acc: 0.2727\n",
      "\n",
      "Epoch 00017: loss did not improve from 1.73823\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7850 - acc: 0.2727\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.73823\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7846 - acc: 0.2727\n",
      "\n",
      "Epoch 00019: loss did not improve from 1.73823\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7841 - acc: 0.2727\n",
      "\n",
      "Epoch 00020: loss did not improve from 1.73823\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7834 - acc: 0.2727\n",
      "\n",
      "Epoch 00021: loss did not improve from 1.73823\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7830 - acc: 0.2727\n",
      "\n",
      "Epoch 00022: loss did not improve from 1.73823\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7825 - acc: 0.2727\n",
      "\n",
      "Epoch 00023: loss did not improve from 1.73823\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7818 - acc: 0.2727\n",
      "\n",
      "Epoch 00024: loss did not improve from 1.73823\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7813 - acc: 0.2727\n",
      "\n",
      "Epoch 00025: loss did not improve from 1.73823\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7805 - acc: 0.2727\n",
      "\n",
      "Epoch 00026: loss did not improve from 1.73823\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7799 - acc: 0.2727\n",
      "\n",
      "Epoch 00027: loss did not improve from 1.73823\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7793 - acc: 0.2727\n",
      "\n",
      "Epoch 00028: loss did not improve from 1.73823\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7788 - acc: 0.2727\n",
      "\n",
      "Epoch 00029: loss did not improve from 1.73823\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7782 - acc: 0.2727\n",
      "\n",
      "Epoch 00030: loss did not improve from 1.73823\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7776 - acc: 0.2727\n",
      "\n",
      "Epoch 00031: loss did not improve from 1.73823\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7769 - acc: 0.2727\n",
      "\n",
      "Epoch 00032: loss did not improve from 1.73823\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7763 - acc: 0.2727\n",
      "\n",
      "Epoch 00033: loss did not improve from 1.73823\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7753 - acc: 0.2727\n",
      "\n",
      "Epoch 00034: loss did not improve from 1.73823\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7746 - acc: 0.2727\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.73823\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7739 - acc: 0.2727\n",
      "\n",
      "Epoch 00036: loss did not improve from 1.73823\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7732 - acc: 0.2727\n",
      "\n",
      "Epoch 00037: loss did not improve from 1.73823\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7725 - acc: 0.2727\n",
      "\n",
      "Epoch 00038: loss did not improve from 1.73823\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7719 - acc: 0.2727\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.73823\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7711 - acc: 0.2727\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.73823\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7706 - acc: 0.2727\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.73823\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7702 - acc: 0.2727\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.73823\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7696 - acc: 0.2727\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.73823\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 1.7691 - acc: 0.2727\n",
      "\n",
      "Epoch 00044: loss did not improve from 1.73823\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7684 - acc: 0.2727\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.73823\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7679 - acc: 0.2727\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.73823\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7674 - acc: 0.2727\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.73823\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7666 - acc: 0.2727\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.73823\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7658 - acc: 0.2727\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.73823\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7654 - acc: 0.2727\n",
      "\n",
      "Epoch 00050: loss did not improve from 1.73823\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7647 - acc: 0.2727\n",
      "\n",
      "Epoch 00051: loss did not improve from 1.73823\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7642 - acc: 0.2727\n",
      "\n",
      "Epoch 00052: loss did not improve from 1.73823\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7635 - acc: 0.2727\n",
      "\n",
      "Epoch 00053: loss did not improve from 1.73823\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7630 - acc: 0.2727\n",
      "\n",
      "Epoch 00054: loss did not improve from 1.73823\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7623 - acc: 0.2727\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.73823\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7614 - acc: 0.2727\n",
      "\n",
      "Epoch 00056: loss did not improve from 1.73823\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7609 - acc: 0.2727\n",
      "\n",
      "Epoch 00057: loss did not improve from 1.73823\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7605 - acc: 0.2727\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.73823\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7596 - acc: 0.2727\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.73823\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7590 - acc: 0.2727\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.73823\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7584 - acc: 0.2727\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.73823\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7577 - acc: 0.2727\n",
      "\n",
      "Epoch 00062: loss did not improve from 1.73823\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7572 - acc: 0.2727\n",
      "\n",
      "Epoch 00063: loss did not improve from 1.73823\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7564 - acc: 0.2727\n",
      "\n",
      "Epoch 00064: loss did not improve from 1.73823\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7560 - acc: 0.2727\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.73823\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7551 - acc: 0.2727\n",
      "\n",
      "Epoch 00066: loss did not improve from 1.73823\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7547 - acc: 0.2727\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.73823\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 2s 226ms/step - loss: 1.7542 - acc: 0.2727\n",
      "\n",
      "Epoch 00068: loss did not improve from 1.73823\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 3s 229ms/step - loss: 1.7534 - acc: 0.2727\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.73823\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 3s 227ms/step - loss: 1.7520 - acc: 0.2727\n",
      "\n",
      "Epoch 00070: loss did not improve from 1.73823\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 2s 226ms/step - loss: 1.7483 - acc: 0.2727\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.73823\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 1.7426 - acc: 0.2727\n",
      "\n",
      "Epoch 00072: loss did not improve from 1.73823\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 1.6792 - acc: 0.2727\n",
      "\n",
      "Epoch 00073: loss improved from 1.73823 to 1.67921, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 1.8773 - acc: 0.2727\n",
      "\n",
      "Epoch 00074: loss did not improve from 1.67921\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 1.6827 - acc: 0.2727\n",
      "\n",
      "Epoch 00075: loss did not improve from 1.67921\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 1.7156 - acc: 0.2727\n",
      "\n",
      "Epoch 00076: loss did not improve from 1.67921\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 1.7052 - acc: 0.2727\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.67921\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 1.6724 - acc: 0.2727\n",
      "\n",
      "Epoch 00078: loss improved from 1.67921 to 1.67241, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 1.8750 - acc: 0.2727\n",
      "\n",
      "Epoch 00079: loss did not improve from 1.67241\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 1.7763 - acc: 0.2727\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.67241\n",
      "Epoch 81/200\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 1.7467 - acc: 0.2727\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.67241\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7483 - acc: 0.2727\n",
      "\n",
      "Epoch 00082: loss did not improve from 1.67241\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7487 - acc: 0.2727\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.67241\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7484 - acc: 0.2727\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.67241\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7481 - acc: 0.2727\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.67241\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7477 - acc: 0.2727\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.67241\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7475 - acc: 0.2727\n",
      "\n",
      "Epoch 00087: loss did not improve from 1.67241\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7469 - acc: 0.2727\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.67241\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7464 - acc: 0.2727\n",
      "\n",
      "Epoch 00089: loss did not improve from 1.67241\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7460 - acc: 0.2727\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.67241\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7454 - acc: 0.2727\n",
      "\n",
      "Epoch 00091: loss did not improve from 1.67241\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7449 - acc: 0.2727\n",
      "\n",
      "Epoch 00092: loss did not improve from 1.67241\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7441 - acc: 0.2727\n",
      "\n",
      "Epoch 00093: loss did not improve from 1.67241\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7436 - acc: 0.2727\n",
      "\n",
      "Epoch 00094: loss did not improve from 1.67241\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7425 - acc: 0.2727\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.67241\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7420 - acc: 0.2727\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.67241\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7414 - acc: 0.2727\n",
      "\n",
      "Epoch 00097: loss did not improve from 1.67241\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7408 - acc: 0.2727\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.67241\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7402 - acc: 0.2727\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.67241\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7396 - acc: 0.2727\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.67241\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7392 - acc: 0.2727\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.67241\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7387 - acc: 0.2727\n",
      "\n",
      "Epoch 00102: loss did not improve from 1.67241\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7382 - acc: 0.2727\n",
      "\n",
      "Epoch 00103: loss did not improve from 1.67241\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7378 - acc: 0.2727\n",
      "\n",
      "Epoch 00104: loss did not improve from 1.67241\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7373 - acc: 0.2727\n",
      "\n",
      "Epoch 00105: loss did not improve from 1.67241\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7368 - acc: 0.2727\n",
      "\n",
      "Epoch 00106: loss did not improve from 1.67241\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7360 - acc: 0.2727\n",
      "\n",
      "Epoch 00107: loss did not improve from 1.67241\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7356 - acc: 0.2727\n",
      "\n",
      "Epoch 00108: loss did not improve from 1.67241\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7350 - acc: 0.2727\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.67241\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7345 - acc: 0.2727\n",
      "\n",
      "Epoch 00110: loss did not improve from 1.67241\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7343 - acc: 0.2727\n",
      "\n",
      "Epoch 00111: loss did not improve from 1.67241\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7338 - acc: 0.2727\n",
      "\n",
      "Epoch 00112: loss did not improve from 1.67241\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7332 - acc: 0.2727\n",
      "\n",
      "Epoch 00113: loss did not improve from 1.67241\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7327 - acc: 0.2727\n",
      "\n",
      "Epoch 00114: loss did not improve from 1.67241\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7322 - acc: 0.2727\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.67241\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7318 - acc: 0.2727\n",
      "\n",
      "Epoch 00116: loss did not improve from 1.67241\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7312 - acc: 0.2727\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.67241\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7307 - acc: 0.2727\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.67241\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7301 - acc: 0.2727\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.67241\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7296 - acc: 0.2727\n",
      "\n",
      "Epoch 00120: loss did not improve from 1.67241\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7294 - acc: 0.2727\n",
      "\n",
      "Epoch 00121: loss did not improve from 1.67241\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7292 - acc: 0.2727\n",
      "\n",
      "Epoch 00122: loss did not improve from 1.67241\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7288 - acc: 0.2727\n",
      "\n",
      "Epoch 00123: loss did not improve from 1.67241\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7281 - acc: 0.2727\n",
      "\n",
      "Epoch 00124: loss did not improve from 1.67241\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7276 - acc: 0.2727\n",
      "\n",
      "Epoch 00125: loss did not improve from 1.67241\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7272 - acc: 0.2727\n",
      "\n",
      "Epoch 00126: loss did not improve from 1.67241\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7264 - acc: 0.2727\n",
      "\n",
      "Epoch 00127: loss did not improve from 1.67241\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7258 - acc: 0.2727\n",
      "\n",
      "Epoch 00128: loss did not improve from 1.67241\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7250 - acc: 0.2727\n",
      "\n",
      "Epoch 00129: loss did not improve from 1.67241\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7242 - acc: 0.2727\n",
      "\n",
      "Epoch 00130: loss did not improve from 1.67241\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7233 - acc: 0.2727\n",
      "\n",
      "Epoch 00131: loss did not improve from 1.67241\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7228 - acc: 0.2727\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.67241\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7223 - acc: 0.2727\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.67241\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7218 - acc: 0.2727\n",
      "\n",
      "Epoch 00134: loss did not improve from 1.67241\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7213 - acc: 0.2727\n",
      "\n",
      "Epoch 00135: loss did not improve from 1.67241\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7209 - acc: 0.2727\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.67241\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7200 - acc: 0.2727\n",
      "\n",
      "Epoch 00137: loss did not improve from 1.67241\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7191 - acc: 0.2727\n",
      "\n",
      "Epoch 00138: loss did not improve from 1.67241\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7181 - acc: 0.2727\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.67241\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7176 - acc: 0.2727\n",
      "\n",
      "Epoch 00140: loss did not improve from 1.67241\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7164 - acc: 0.2727\n",
      "\n",
      "Epoch 00141: loss did not improve from 1.67241\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7160 - acc: 0.2727\n",
      "\n",
      "Epoch 00142: loss did not improve from 1.67241\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7148 - acc: 0.2727\n",
      "\n",
      "Epoch 00143: loss did not improve from 1.67241\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7142 - acc: 0.2727\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.67241\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7136 - acc: 0.2727\n",
      "\n",
      "Epoch 00145: loss did not improve from 1.67241\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7130 - acc: 0.2727\n",
      "\n",
      "Epoch 00146: loss did not improve from 1.67241\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7124 - acc: 0.2727\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.67241\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7114 - acc: 0.2727\n",
      "\n",
      "Epoch 00148: loss did not improve from 1.67241\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7113 - acc: 0.2727\n",
      "\n",
      "Epoch 00149: loss did not improve from 1.67241\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7104 - acc: 0.2727\n",
      "\n",
      "Epoch 00150: loss did not improve from 1.67241\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7099 - acc: 0.2727\n",
      "\n",
      "Epoch 00151: loss did not improve from 1.67241\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7097 - acc: 0.2727\n",
      "\n",
      "Epoch 00152: loss did not improve from 1.67241\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7093 - acc: 0.2727\n",
      "\n",
      "Epoch 00153: loss did not improve from 1.67241\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7087 - acc: 0.2727\n",
      "\n",
      "Epoch 00154: loss did not improve from 1.67241\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7083 - acc: 0.2727\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.67241\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7077 - acc: 0.2727\n",
      "\n",
      "Epoch 00156: loss did not improve from 1.67241\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.7072 - acc: 0.2727\n",
      "\n",
      "Epoch 00157: loss did not improve from 1.67241\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7059 - acc: 0.2727\n",
      "\n",
      "Epoch 00158: loss did not improve from 1.67241\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7057 - acc: 0.2727\n",
      "\n",
      "Epoch 00159: loss did not improve from 1.67241\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7047 - acc: 0.2727\n",
      "\n",
      "Epoch 00160: loss did not improve from 1.67241\n",
      "Epoch 161/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7043 - acc: 0.2727\n",
      "\n",
      "Epoch 00161: loss did not improve from 1.67241\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7039 - acc: 0.2727\n",
      "\n",
      "Epoch 00162: loss did not improve from 1.67241\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7034 - acc: 0.2727\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.67241\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7031 - acc: 0.2727\n",
      "\n",
      "Epoch 00164: loss did not improve from 1.67241\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7028 - acc: 0.2727\n",
      "\n",
      "Epoch 00165: loss did not improve from 1.67241\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7027 - acc: 0.2727\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.67241\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7022 - acc: 0.2727\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.67241\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7017 - acc: 0.2727\n",
      "\n",
      "Epoch 00168: loss did not improve from 1.67241\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7015 - acc: 0.2727\n",
      "\n",
      "Epoch 00169: loss did not improve from 1.67241\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.7006 - acc: 0.2727\n",
      "\n",
      "Epoch 00170: loss did not improve from 1.67241\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6997 - acc: 0.2727\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.67241\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6992 - acc: 0.2727\n",
      "\n",
      "Epoch 00172: loss did not improve from 1.67241\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6988 - acc: 0.2727\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.67241\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6979 - acc: 0.2727\n",
      "\n",
      "Epoch 00174: loss did not improve from 1.67241\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6975 - acc: 0.2727\n",
      "\n",
      "Epoch 00175: loss did not improve from 1.67241\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6975 - acc: 0.2727\n",
      "\n",
      "Epoch 00176: loss did not improve from 1.67241\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6973 - acc: 0.2727\n",
      "\n",
      "Epoch 00177: loss did not improve from 1.67241\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6969 - acc: 0.2727\n",
      "\n",
      "Epoch 00178: loss did not improve from 1.67241\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6967 - acc: 0.2727\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.67241\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6964 - acc: 0.2727\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.67241\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6957 - acc: 0.2727\n",
      "\n",
      "Epoch 00181: loss did not improve from 1.67241\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6956 - acc: 0.2727\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.67241\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6945 - acc: 0.2727\n",
      "\n",
      "Epoch 00183: loss did not improve from 1.67241\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6943 - acc: 0.2727\n",
      "\n",
      "Epoch 00184: loss did not improve from 1.67241\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6939 - acc: 0.2727\n",
      "\n",
      "Epoch 00185: loss did not improve from 1.67241\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6937 - acc: 0.2727\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.67241\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6933 - acc: 0.2727\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.67241\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6931 - acc: 0.2727\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.67241\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6929 - acc: 0.2727\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.67241\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6925 - acc: 0.2727\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.67241\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6922 - acc: 0.2727\n",
      "\n",
      "Epoch 00191: loss did not improve from 1.67241\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6918 - acc: 0.2727\n",
      "\n",
      "Epoch 00192: loss did not improve from 1.67241\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6914 - acc: 0.2727\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.67241\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6908 - acc: 0.2727\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.67241\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6903 - acc: 0.2727\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.67241\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6902 - acc: 0.2727\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.67241\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 1.6895 - acc: 0.2727\n",
      "\n",
      "Epoch 00197: loss did not improve from 1.67241\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6891 - acc: 0.2727\n",
      "\n",
      "Epoch 00198: loss did not improve from 1.67241\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6886 - acc: 0.2727\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.67241\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 1.6885 - acc: 0.2727\n",
      "\n",
      "Epoch 00200: loss did not improve from 1.67241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c7489287f0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 5, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 23962)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 23962, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 23962, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 23962, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 23962, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 23962, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 23962, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 23962, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 23962, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 23962, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 23962, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 23962, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 23962, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 23962, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 23962, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 23962, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 23962, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 23962, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 23962, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 23962, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 23962, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 23962, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 23962, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 23962, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 23962, 40)    0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 23962, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 23962, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 23962, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 23962, 40)    0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 23962, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 23962, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 23962, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 23962, 40)    0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 23962, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_tanh (Conv1D)  (None, 23962, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_sigm (Conv1D)  (None, 23962, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 23962, 40)    0           dilated_conv_512_tanh[0][0]      \n",
      "                                                                 dilated_conv_512_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 23962, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 23962, 40)    128040      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 300, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 300, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 300, 10)      40010       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 10)        0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 10)        210         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 10)        0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 10)           0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 10)           0           reshape_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 401,500\n",
      "Trainable params: 401,500\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1 sample for 1 user\n",
    "# no_users = 10\n",
    "\n",
    "wnc = WaveNetClassifier((23962,), (10,), kernel_size = 2, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 9s 862ms/step - loss: 2.3025 - acc: 0.1000\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.30248, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 2.3022 - acc: 0.1000\n",
      "\n",
      "Epoch 00002: loss improved from 2.30248 to 2.30217, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 2.2996 - acc: 0.1000\n",
      "\n",
      "Epoch 00003: loss improved from 2.30217 to 2.29960, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00004: loss did not improve from 2.29960\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 2.2900 - acc: 0.2000\n",
      "\n",
      "Epoch 00005: loss improved from 2.29960 to 2.29002, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 2.2853 - acc: 0.1000\n",
      "\n",
      "Epoch 00006: loss improved from 2.29002 to 2.28526, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 2.2609 - acc: 0.3000\n",
      "\n",
      "Epoch 00007: loss improved from 2.28526 to 2.26095, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 2.2207 - acc: 0.2000\n",
      "\n",
      "Epoch 00008: loss improved from 2.26095 to 2.22067, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 2.1846 - acc: 0.2000\n",
      "\n",
      "Epoch 00009: loss improved from 2.22067 to 2.18465, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 2.1326 - acc: 0.3000\n",
      "\n",
      "Epoch 00010: loss improved from 2.18465 to 2.13261, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 2.0819 - acc: 0.4000\n",
      "\n",
      "Epoch 00011: loss improved from 2.13261 to 2.08187, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 2.0219 - acc: 0.4000\n",
      "\n",
      "Epoch 00012: loss improved from 2.08187 to 2.02191, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.9397 - acc: 0.4000\n",
      "\n",
      "Epoch 00013: loss improved from 2.02191 to 1.93974, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.8200 - acc: 0.4000\n",
      "\n",
      "Epoch 00014: loss improved from 1.93974 to 1.82004, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.6618 - acc: 0.5000\n",
      "\n",
      "Epoch 00015: loss improved from 1.82004 to 1.66180, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.4778 - acc: 0.5000\n",
      "\n",
      "Epoch 00016: loss improved from 1.66180 to 1.47781, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.3756 - acc: 0.5000\n",
      "\n",
      "Epoch 00017: loss improved from 1.47781 to 1.37556, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.5302 - acc: 0.6000\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.37556\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.2298 - acc: 0.7000\n",
      "\n",
      "Epoch 00019: loss improved from 1.37556 to 1.22979, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.3583 - acc: 0.6000\n",
      "\n",
      "Epoch 00020: loss did not improve from 1.22979\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0827 - acc: 0.8000\n",
      "\n",
      "Epoch 00021: loss improved from 1.22979 to 1.08268, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0814 - acc: 0.8000\n",
      "\n",
      "Epoch 00022: loss improved from 1.08268 to 1.08143, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0232 - acc: 0.8000\n",
      "\n",
      "Epoch 00023: loss improved from 1.08143 to 1.02323, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.8916 - acc: 0.8000\n",
      "\n",
      "Epoch 00024: loss improved from 1.02323 to 0.89156, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.8560 - acc: 0.8000\n",
      "\n",
      "Epoch 00025: loss improved from 0.89156 to 0.85604, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.7710 - acc: 0.7000\n",
      "\n",
      "Epoch 00026: loss improved from 0.85604 to 0.77105, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.7107 - acc: 0.8000\n",
      "\n",
      "Epoch 00027: loss improved from 0.77105 to 0.71065, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6923 - acc: 0.8000\n",
      "\n",
      "Epoch 00028: loss improved from 0.71065 to 0.69227, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6499 - acc: 0.8000\n",
      "\n",
      "Epoch 00029: loss improved from 0.69227 to 0.64988, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6148 - acc: 0.8000\n",
      "\n",
      "Epoch 00030: loss improved from 0.64988 to 0.61480, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5821 - acc: 0.7000\n",
      "\n",
      "Epoch 00031: loss improved from 0.61480 to 0.58215, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5327 - acc: 0.7000\n",
      "\n",
      "Epoch 00032: loss improved from 0.58215 to 0.53270, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4982 - acc: 0.7000\n",
      "\n",
      "Epoch 00033: loss improved from 0.53270 to 0.49818, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4393 - acc: 0.8000\n",
      "\n",
      "Epoch 00034: loss improved from 0.49818 to 0.43927, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.3700 - acc: 0.9000\n",
      "\n",
      "Epoch 00035: loss improved from 0.43927 to 0.37004, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3296 - acc: 0.9000\n",
      "\n",
      "Epoch 00036: loss improved from 0.37004 to 0.32956, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.2733 - acc: 0.9000\n",
      "\n",
      "Epoch 00037: loss improved from 0.32956 to 0.27330, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3038 - acc: 0.9000\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.27330\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.6318 - acc: 0.6000\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.27330\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.4306 - acc: 0.7000\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.27330\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 2.2389 - acc: 0.4000\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.27330\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.8515 - acc: 0.7000\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.27330\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 8.8410 - acc: 0.2000\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.27330\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0323 - acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: loss did not improve from 0.27330\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.1061 - acc: 0.6000\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.27330\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.3860 - acc: 0.3000\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.27330\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.5713 - acc: 0.4000\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.27330\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.4892 - acc: 0.4000\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.27330\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.3818 - acc: 0.4000\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.27330\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.2392 - acc: 0.4000\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.27330\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.1535 - acc: 0.5000\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.27330\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0193 - acc: 0.7000\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.27330\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9962 - acc: 0.6000\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.27330\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.8931 - acc: 0.7000\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.27330\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8139 - acc: 0.7000\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.27330\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8107 - acc: 0.6000\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.27330\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7633 - acc: 0.6000\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.27330\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7055 - acc: 0.7000\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.27330\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6531 - acc: 0.7000\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.27330\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5445 - acc: 0.8000\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.27330\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4385 - acc: 0.9000\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.27330\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4371 - acc: 0.9000\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.27330\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4695 - acc: 0.8000\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.27330\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.3925 - acc: 0.9000\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.27330\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3555 - acc: 0.9000\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.27330\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3095 - acc: 0.9000\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.27330\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.2944 - acc: 0.9000\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.27330\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.2762 - acc: 0.9000\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.27330\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.2638 - acc: 0.9000\n",
      "\n",
      "Epoch 00069: loss improved from 0.27330 to 0.26382, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2488 - acc: 0.9000\n",
      "\n",
      "Epoch 00070: loss improved from 0.26382 to 0.24880, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2339 - acc: 0.9000\n",
      "\n",
      "Epoch 00071: loss improved from 0.24880 to 0.23388, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2203 - acc: 0.9000\n",
      "\n",
      "Epoch 00072: loss improved from 0.23388 to 0.22034, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2103 - acc: 0.9000\n",
      "\n",
      "Epoch 00073: loss improved from 0.22034 to 0.21034, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2034 - acc: 0.9000\n",
      "\n",
      "Epoch 00074: loss improved from 0.21034 to 0.20340, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1941 - acc: 0.9000\n",
      "\n",
      "Epoch 00075: loss improved from 0.20340 to 0.19414, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1839 - acc: 0.9000\n",
      "\n",
      "Epoch 00076: loss improved from 0.19414 to 0.18391, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1768 - acc: 0.9000\n",
      "\n",
      "Epoch 00077: loss improved from 0.18391 to 0.17681, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.1709 - acc: 0.9000\n",
      "\n",
      "Epoch 00078: loss improved from 0.17681 to 0.17090, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.1619 - acc: 0.9000\n",
      "\n",
      "Epoch 00079: loss improved from 0.17090 to 0.16190, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1564 - acc: 1.0000\n",
      "\n",
      "Epoch 00080: loss improved from 0.16190 to 0.15639, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1512 - acc: 1.0000\n",
      "\n",
      "Epoch 00081: loss improved from 0.15639 to 0.15117, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.1442 - acc: 0.9000\n",
      "\n",
      "Epoch 00082: loss improved from 0.15117 to 0.14422, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.1408 - acc: 0.9000\n",
      "\n",
      "Epoch 00083: loss improved from 0.14422 to 0.14084, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.1351 - acc: 1.0000\n",
      "\n",
      "Epoch 00084: loss improved from 0.14084 to 0.13514, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1330 - acc: 1.0000\n",
      "\n",
      "Epoch 00085: loss improved from 0.13514 to 0.13304, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1306 - acc: 1.0000\n",
      "\n",
      "Epoch 00086: loss improved from 0.13304 to 0.13059, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1276 - acc: 1.0000\n",
      "\n",
      "Epoch 00087: loss improved from 0.13059 to 0.12761, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.1240 - acc: 1.0000\n",
      "\n",
      "Epoch 00088: loss improved from 0.12761 to 0.12401, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1191 - acc: 1.0000\n",
      "\n",
      "Epoch 00089: loss improved from 0.12401 to 0.11909, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1146 - acc: 1.0000\n",
      "\n",
      "Epoch 00090: loss improved from 0.11909 to 0.11460, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1095 - acc: 1.0000\n",
      "\n",
      "Epoch 00091: loss improved from 0.11460 to 0.10953, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1035 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00092: loss improved from 0.10953 to 0.10345, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.0983 - acc: 1.0000\n",
      "\n",
      "Epoch 00093: loss improved from 0.10345 to 0.09828, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.0952 - acc: 1.0000\n",
      "\n",
      "Epoch 00094: loss improved from 0.09828 to 0.09517, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.1114 - acc: 0.9000\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.09517\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.1144 - acc: 0.9000\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.09517\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.1015 - acc: 0.9000\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.09517\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.0749 - acc: 1.0000\n",
      "\n",
      "Epoch 00098: loss improved from 0.09517 to 0.07489, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.0982 - acc: 0.9000\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.07489\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.0703 - acc: 1.0000\n",
      "\n",
      "Epoch 00100: loss improved from 0.07489 to 0.07035, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.0740 - acc: 1.0000\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.07035\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.0690 - acc: 1.0000\n",
      "\n",
      "Epoch 00102: loss improved from 0.07035 to 0.06897, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0517 - acc: 1.0000\n",
      "\n",
      "Epoch 00103: loss improved from 0.06897 to 0.05171, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0589 - acc: 1.0000\n",
      "\n",
      "Epoch 00104: loss did not improve from 0.05171\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.0404 - acc: 1.0000\n",
      "\n",
      "Epoch 00105: loss improved from 0.05171 to 0.04039, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0530 - acc: 1.0000\n",
      "\n",
      "Epoch 00106: loss did not improve from 0.04039\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0896 - acc: 0.9000\n",
      "\n",
      "Epoch 00107: loss did not improve from 0.04039\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0324 - acc: 1.0000\n",
      "\n",
      "Epoch 00108: loss improved from 0.04039 to 0.03239, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0758 - acc: 1.0000\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.03239\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0546 - acc: 1.0000\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.03239\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0580 - acc: 1.0000\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.03239\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0336 - acc: 1.0000\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.03239\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0629 - acc: 1.0000\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.03239\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0197 - acc: 1.0000\n",
      "\n",
      "Epoch 00114: loss improved from 0.03239 to 0.01971, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.0457 - acc: 1.0000\n",
      "\n",
      "Epoch 00115: loss did not improve from 0.01971\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.0273 - acc: 1.0000\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.01971\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0167 - acc: 1.0000\n",
      "\n",
      "Epoch 00117: loss improved from 0.01971 to 0.01666, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0327 - acc: 1.0000\n",
      "\n",
      "Epoch 00118: loss did not improve from 0.01666\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0189 - acc: 1.0000\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.01666\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0117 - acc: 1.0000\n",
      "\n",
      "Epoch 00120: loss improved from 0.01666 to 0.01172, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0187 - acc: 1.0000\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.01172\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0174 - acc: 1.0000\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.01172\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0093 - acc: 1.0000\n",
      "\n",
      "Epoch 00123: loss improved from 0.01172 to 0.00932, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0078 - acc: 1.0000\n",
      "\n",
      "Epoch 00124: loss improved from 0.00932 to 0.00777, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0104 - acc: 1.0000\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.00777\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0107 - acc: 1.0000\n",
      "\n",
      "Epoch 00126: loss did not improve from 0.00777\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.0069 - acc: 1.0000\n",
      "\n",
      "Epoch 00127: loss improved from 0.00777 to 0.00693, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0047 - acc: 1.0000\n",
      "\n",
      "Epoch 00128: loss improved from 0.00693 to 0.00466, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0044 - acc: 1.0000\n",
      "\n",
      "Epoch 00129: loss improved from 0.00466 to 0.00444, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0051 - acc: 1.0000\n",
      "\n",
      "Epoch 00130: loss did not improve from 0.00444\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0054 - acc: 1.0000\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.00444\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0045 - acc: 1.0000\n",
      "\n",
      "Epoch 00132: loss did not improve from 0.00444\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0033 - acc: 1.0000\n",
      "\n",
      "Epoch 00133: loss improved from 0.00444 to 0.00330, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-1903f6f430f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    215\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m   1088\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[0m_serialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[1;34m(model, f, include_optimizer)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mlayer_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_weights_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0msymbolic_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mweight_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mweight_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[1;34m(ops)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \"\"\"\n\u001b[0;32m   2419\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 32, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 18330)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 18330, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 18330, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 18330, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 18330, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 18330, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 18330, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 18330, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 18330, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 18330, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 18330, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 18330, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 18330, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 18330, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 18330, 40)    0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 18330, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 18330, 40)    0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 18330, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 18330, 40)    0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 18330, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 18330, 40)    0           dilated_conv_512_tanh[0][0]      \n",
      "                                                                 dilated_conv_512_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 18330, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 18330, 40)    128040      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 230, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 230, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 230, 20)      80020       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 20)        0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 20)        820         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 20)        0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 20)           0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20)           0           reshape_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 442,120\n",
      "Trainable params: 442,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1 sample for 1 user\n",
    "# no_users = 20\n",
    "\n",
    "wnc = WaveNetClassifier((18330,), (20,), kernel_size = 2, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20/20 [==============================] - 9s 438ms/step - loss: 2.9955 - acc: 0.0500\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.99554, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 2.9962 - acc: 0.0500\n",
      "\n",
      "Epoch 00002: loss did not improve from 2.99554\n",
      "Epoch 3/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.9927 - acc: 0.0500\n",
      "\n",
      "Epoch 00003: loss improved from 2.99554 to 2.99274, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.9888 - acc: 0.0500\n",
      "\n",
      "Epoch 00004: loss improved from 2.99274 to 2.98877, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 5/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.9783 - acc: 0.0500\n",
      "\n",
      "Epoch 00005: loss improved from 2.98877 to 2.97829, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.9458 - acc: 0.0500\n",
      "\n",
      "Epoch 00006: loss improved from 2.97829 to 2.94578, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 7/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.9342 - acc: 0.0500\n",
      "\n",
      "Epoch 00007: loss improved from 2.94578 to 2.93415, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.8699 - acc: 0.0500\n",
      "\n",
      "Epoch 00008: loss improved from 2.93415 to 2.86987, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 9/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.8475 - acc: 0.0500\n",
      "\n",
      "Epoch 00009: loss improved from 2.86987 to 2.84749, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 10/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.7942 - acc: 0.0500\n",
      "\n",
      "Epoch 00010: loss improved from 2.84749 to 2.79419, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 11/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.7475 - acc: 0.1000\n",
      "\n",
      "Epoch 00011: loss improved from 2.79419 to 2.74752, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 12/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.7252 - acc: 0.0500\n",
      "\n",
      "Epoch 00012: loss improved from 2.74752 to 2.72524, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 13/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.6870 - acc: 0.2000\n",
      "\n",
      "Epoch 00013: loss improved from 2.72524 to 2.68698, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 14/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.6497 - acc: 0.2000\n",
      "\n",
      "Epoch 00014: loss improved from 2.68698 to 2.64969, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 15/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.5992 - acc: 0.2500\n",
      "\n",
      "Epoch 00015: loss improved from 2.64969 to 2.59922, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 16/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.5383 - acc: 0.3000\n",
      "\n",
      "Epoch 00016: loss improved from 2.59922 to 2.53827, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 17/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.4757 - acc: 0.2500\n",
      "\n",
      "Epoch 00017: loss improved from 2.53827 to 2.47569, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 18/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.3714 - acc: 0.2500\n",
      "\n",
      "Epoch 00018: loss improved from 2.47569 to 2.37145, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 19/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.2967 - acc: 0.2500\n",
      "\n",
      "Epoch 00019: loss improved from 2.37145 to 2.29668, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 20/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.2283 - acc: 0.3000\n",
      "\n",
      "Epoch 00020: loss improved from 2.29668 to 2.22831, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 21/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2.0365 - acc: 0.4000\n",
      "\n",
      "Epoch 00021: loss improved from 2.22831 to 2.03650, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 22/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.9897 - acc: 0.3500\n",
      "\n",
      "Epoch 00022: loss improved from 2.03650 to 1.98972, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 23/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.9138 - acc: 0.4000\n",
      "\n",
      "Epoch 00023: loss improved from 1.98972 to 1.91378, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.6726 - acc: 0.4500\n",
      "\n",
      "Epoch 00024: loss improved from 1.91378 to 1.67261, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 25/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.6835 - acc: 0.5000\n",
      "\n",
      "Epoch 00025: loss did not improve from 1.67261\n",
      "Epoch 26/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.5074 - acc: 0.5000\n",
      "\n",
      "Epoch 00026: loss improved from 1.67261 to 1.50745, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 27/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.4417 - acc: 0.4500\n",
      "\n",
      "Epoch 00027: loss improved from 1.50745 to 1.44166, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 28/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.3722 - acc: 0.5500\n",
      "\n",
      "Epoch 00028: loss improved from 1.44166 to 1.37216, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 29/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.1771 - acc: 0.6500\n",
      "\n",
      "Epoch 00029: loss improved from 1.37216 to 1.17715, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 30/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.2134 - acc: 0.6500\n",
      "\n",
      "Epoch 00030: loss did not improve from 1.17715\n",
      "Epoch 31/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.1207 - acc: 0.6000\n",
      "\n",
      "Epoch 00031: loss improved from 1.17715 to 1.12072, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 32/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.8677 - acc: 0.7500\n",
      "\n",
      "Epoch 00032: loss improved from 1.12072 to 0.86773, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 33/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.8806 - acc: 0.7000\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.86773\n",
      "Epoch 34/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.7108 - acc: 0.8500\n",
      "\n",
      "Epoch 00034: loss improved from 0.86773 to 0.71075, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 35/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.5551 - acc: 0.8500\n",
      "\n",
      "Epoch 00035: loss improved from 0.71075 to 0.55508, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 36/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.5906 - acc: 0.7500\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.55508\n",
      "Epoch 37/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.4286 - acc: 0.8500\n",
      "\n",
      "Epoch 00037: loss improved from 0.55508 to 0.42863, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 38/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.3865 - acc: 0.9000\n",
      "\n",
      "Epoch 00038: loss improved from 0.42863 to 0.38654, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 39/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.3634 - acc: 0.9000\n",
      "\n",
      "Epoch 00039: loss improved from 0.38654 to 0.36340, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 40/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2782 - acc: 0.9000\n",
      "\n",
      "Epoch 00040: loss improved from 0.36340 to 0.27816, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 41/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2874 - acc: 0.9000\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.27816\n",
      "Epoch 42/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2828 - acc: 0.9000\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.27816\n",
      "Epoch 43/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1860 - acc: 1.0000\n",
      "\n",
      "Epoch 00043: loss improved from 0.27816 to 0.18597, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2339 - acc: 0.9000\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.18597\n",
      "Epoch 45/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.3065 - acc: 0.8500\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.18597\n",
      "Epoch 46/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1314 - acc: 0.9500\n",
      "\n",
      "Epoch 00046: loss improved from 0.18597 to 0.13136, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 47/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2093 - acc: 0.9500\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.13136\n",
      "Epoch 48/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1001 - acc: 1.0000\n",
      "\n",
      "Epoch 00048: loss improved from 0.13136 to 0.10015, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 49/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1780 - acc: 0.9000\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.10015\n",
      "Epoch 50/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.8065 - acc: 0.8500\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.10015\n",
      "Epoch 51/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.7659 - acc: 0.8000\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.10015\n",
      "Epoch 52/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.3745 - acc: 0.8500\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.10015\n",
      "Epoch 53/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1412 - acc: 1.0000\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.10015\n",
      "Epoch 54/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.9007 - acc: 0.8000\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.10015\n",
      "Epoch 55/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.4151 - acc: 0.8000\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.10015\n",
      "Epoch 56/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.3708 - acc: 0.9000\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.10015\n",
      "Epoch 57/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.4008 - acc: 0.9500\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.10015\n",
      "Epoch 58/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2999 - acc: 0.8500\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.10015\n",
      "Epoch 59/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.9808 - acc: 0.6500\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.10015\n",
      "Epoch 60/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 1.1150 - acc: 0.5500\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.10015\n",
      "Epoch 61/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1775 - acc: 0.9500\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.10015\n",
      "Epoch 62/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.4811 - acc: 0.9000\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.10015\n",
      "Epoch 63/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.4541 - acc: 0.8500\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.10015\n",
      "Epoch 64/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1666 - acc: 0.9000\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.10015\n",
      "Epoch 65/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.4062 - acc: 0.7500\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.10015\n",
      "Epoch 66/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2787 - acc: 0.8500\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.10015\n",
      "Epoch 67/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1714 - acc: 0.9500\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.10015\n",
      "Epoch 68/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.3226 - acc: 0.8500\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.10015\n",
      "Epoch 69/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2822 - acc: 0.8500\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.10015\n",
      "Epoch 70/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1478 - acc: 1.0000\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.10015\n",
      "Epoch 71/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1755 - acc: 0.9500\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.10015\n",
      "Epoch 72/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.2472 - acc: 0.9500\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.10015\n",
      "Epoch 73/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1618 - acc: 0.9500\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.10015\n",
      "Epoch 74/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1329 - acc: 0.9500\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.10015\n",
      "Epoch 75/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1797 - acc: 0.9500\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.10015\n",
      "Epoch 76/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1762 - acc: 0.9500\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.10015\n",
      "Epoch 77/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1171 - acc: 0.9500\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.10015\n",
      "Epoch 78/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0947 - acc: 1.0000\n",
      "\n",
      "Epoch 00078: loss improved from 0.10015 to 0.09472, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 79/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1244 - acc: 1.0000\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.09472\n",
      "Epoch 80/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.1085 - acc: 1.0000\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.09472\n",
      "Epoch 81/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0749 - acc: 1.0000\n",
      "\n",
      "Epoch 00081: loss improved from 0.09472 to 0.07490, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 82/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0828 - acc: 1.0000\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.07490\n",
      "Epoch 83/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0941 - acc: 1.0000\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.07490\n",
      "Epoch 84/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0644 - acc: 1.0000\n",
      "\n",
      "Epoch 00084: loss improved from 0.07490 to 0.06437, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 85/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0528 - acc: 1.0000\n",
      "\n",
      "Epoch 00085: loss improved from 0.06437 to 0.05279, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 86/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0666 - acc: 1.0000\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.05279\n",
      "Epoch 87/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0526 - acc: 1.0000\n",
      "\n",
      "Epoch 00087: loss improved from 0.05279 to 0.05263, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 88/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0421 - acc: 1.0000\n",
      "\n",
      "Epoch 00088: loss improved from 0.05263 to 0.04207, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 89/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0507 - acc: 1.0000\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.04207\n",
      "Epoch 90/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0415 - acc: 1.0000\n",
      "\n",
      "Epoch 00090: loss improved from 0.04207 to 0.04152, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 91/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0303 - acc: 1.0000\n",
      "\n",
      "Epoch 00091: loss improved from 0.04152 to 0.03026, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 92/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0346 - acc: 1.0000\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.03026\n",
      "Epoch 93/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0344 - acc: 1.0000\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.03026\n",
      "Epoch 94/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0241 - acc: 1.0000\n",
      "\n",
      "Epoch 00094: loss improved from 0.03026 to 0.02409, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 95/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0213 - acc: 1.0000\n",
      "\n",
      "Epoch 00095: loss improved from 0.02409 to 0.02130, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 96/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0221 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00096: loss did not improve from 0.02130\n",
      "Epoch 97/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0183 - acc: 1.0000\n",
      "\n",
      "Epoch 00097: loss improved from 0.02130 to 0.01835, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 98/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0138 - acc: 1.0000\n",
      "\n",
      "Epoch 00098: loss improved from 0.01835 to 0.01381, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 99/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0139 - acc: 1.0000\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.01381\n",
      "Epoch 100/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0147 - acc: 1.0000\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.01381\n",
      "Epoch 101/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0116 - acc: 1.0000\n",
      "\n",
      "Epoch 00101: loss improved from 0.01381 to 0.01163, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 102/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0084 - acc: 1.0000\n",
      "\n",
      "Epoch 00102: loss improved from 0.01163 to 0.00843, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 103/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0076 - acc: 1.0000\n",
      "\n",
      "Epoch 00103: loss improved from 0.00843 to 0.00755, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 104/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0079 - acc: 1.0000\n",
      "\n",
      "Epoch 00104: loss did not improve from 0.00755\n",
      "Epoch 105/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0078 - acc: 1.0000\n",
      "\n",
      "Epoch 00105: loss did not improve from 0.00755\n",
      "Epoch 106/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0067 - acc: 1.0000\n",
      "\n",
      "Epoch 00106: loss improved from 0.00755 to 0.00672, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 107/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0055 - acc: 1.0000\n",
      "\n",
      "Epoch 00107: loss improved from 0.00672 to 0.00547, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 108/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0048 - acc: 1.0000\n",
      "\n",
      "Epoch 00108: loss improved from 0.00547 to 0.00475, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 109/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0046 - acc: 1.0000\n",
      "\n",
      "Epoch 00109: loss improved from 0.00475 to 0.00459, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 110/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0045 - acc: 1.0000\n",
      "\n",
      "Epoch 00110: loss improved from 0.00459 to 0.00449, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 111/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0041 - acc: 1.0000\n",
      "\n",
      "Epoch 00111: loss improved from 0.00449 to 0.00409, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 112/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0035 - acc: 1.0000\n",
      "\n",
      "Epoch 00112: loss improved from 0.00409 to 0.00351, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 113/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0030 - acc: 1.0000\n",
      "\n",
      "Epoch 00113: loss improved from 0.00351 to 0.00304, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 114/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0028 - acc: 1.0000\n",
      "\n",
      "Epoch 00114: loss improved from 0.00304 to 0.00281, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 115/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0028 - acc: 1.0000\n",
      "\n",
      "Epoch 00115: loss improved from 0.00281 to 0.00276, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 116/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0028 - acc: 1.0000\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.00276\n",
      "Epoch 117/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0027 - acc: 1.0000\n",
      "\n",
      "Epoch 00117: loss improved from 0.00276 to 0.00269, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 118/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0025 - acc: 1.0000\n",
      "\n",
      "Epoch 00118: loss improved from 0.00269 to 0.00249, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 119/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0022 - acc: 1.0000\n",
      "\n",
      "Epoch 00119: loss improved from 0.00249 to 0.00223, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 120/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0020 - acc: 1.0000\n",
      "\n",
      "Epoch 00120: loss improved from 0.00223 to 0.00200, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 121/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0019 - acc: 1.0000\n",
      "\n",
      "Epoch 00121: loss improved from 0.00200 to 0.00185, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 122/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0018 - acc: 1.0000\n",
      "\n",
      "Epoch 00122: loss improved from 0.00185 to 0.00177, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 123/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0017 - acc: 1.0000\n",
      "\n",
      "Epoch 00123: loss improved from 0.00177 to 0.00172, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 124/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0017 - acc: 1.0000\n",
      "\n",
      "Epoch 00124: loss improved from 0.00172 to 0.00168, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 125/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0016 - acc: 1.0000\n",
      "\n",
      "Epoch 00125: loss improved from 0.00168 to 0.00163, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 126/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0016 - acc: 1.0000\n",
      "\n",
      "Epoch 00126: loss improved from 0.00163 to 0.00156, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 127/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0015 - acc: 1.0000\n",
      "\n",
      "Epoch 00127: loss improved from 0.00156 to 0.00148, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 128/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0014 - acc: 1.0000\n",
      "\n",
      "Epoch 00128: loss improved from 0.00148 to 0.00140, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 129/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0013 - acc: 1.0000\n",
      "\n",
      "Epoch 00129: loss improved from 0.00140 to 0.00133, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 130/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0013 - acc: 1.0000\n",
      "\n",
      "Epoch 00130: loss improved from 0.00133 to 0.00128, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 131/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0012 - acc: 1.0000\n",
      "\n",
      "Epoch 00131: loss improved from 0.00128 to 0.00124, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 132/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0012 - acc: 1.0000\n",
      "\n",
      "Epoch 00132: loss improved from 0.00124 to 0.00121, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 133/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0012 - acc: 1.0000\n",
      "\n",
      "Epoch 00133: loss improved from 0.00121 to 0.00118, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 134/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0011 - acc: 1.0000\n",
      "\n",
      "Epoch 00134: loss improved from 0.00118 to 0.00115, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 135/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0011 - acc: 1.0000\n",
      "\n",
      "Epoch 00135: loss improved from 0.00115 to 0.00112, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 136/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0011 - acc: 1.0000\n",
      "\n",
      "Epoch 00136: loss improved from 0.00112 to 0.00108, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 137/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0010 - acc: 1.0000\n",
      "\n",
      "Epoch 00137: loss improved from 0.00108 to 0.00104, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 138/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.0010 - acc: 1.0000\n",
      "\n",
      "Epoch 00138: loss improved from 0.00104 to 0.00100, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 9.7220e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00139: loss improved from 0.00100 to 0.00097, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 140/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 9.4536e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00140: loss improved from 0.00097 to 0.00095, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 141/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 9.2363e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00141: loss improved from 0.00095 to 0.00092, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 142/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 9.0555e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00142: loss improved from 0.00092 to 0.00091, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 143/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 8.8881e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00143: loss improved from 0.00091 to 0.00089, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 144/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 8.7171e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00144: loss improved from 0.00089 to 0.00087, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 145/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 8.5325e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00145: loss improved from 0.00087 to 0.00085, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 146/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 8.3344e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00146: loss improved from 0.00085 to 0.00083, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 147/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 8.1314e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00147: loss improved from 0.00083 to 0.00081, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 148/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 7.9346e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00148: loss improved from 0.00081 to 0.00079, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 149/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 7.7553e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00149: loss improved from 0.00079 to 0.00078, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 150/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 7.5944e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00150: loss improved from 0.00078 to 0.00076, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 151/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 7.4504e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00151: loss improved from 0.00076 to 0.00075, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 152/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 7.3191e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00152: loss improved from 0.00075 to 0.00073, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 153/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 7.1960e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00153: loss improved from 0.00073 to 0.00072, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 154/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 7.0766e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00154: loss improved from 0.00072 to 0.00071, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 155/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.9573e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00155: loss improved from 0.00071 to 0.00070, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 156/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.8371e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00156: loss improved from 0.00070 to 0.00068, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 157/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.7160e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00157: loss improved from 0.00068 to 0.00067, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 158/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.5959e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00158: loss improved from 0.00067 to 0.00066, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 159/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.4788e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00159: loss improved from 0.00066 to 0.00065, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 160/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.3669e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00160: loss improved from 0.00065 to 0.00064, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 161/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.2613e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00161: loss improved from 0.00064 to 0.00063, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 162/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.1614e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00162: loss improved from 0.00063 to 0.00062, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 163/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 6.0676e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00163: loss improved from 0.00062 to 0.00061, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 164/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.9784e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00164: loss improved from 0.00061 to 0.00060, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 165/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.8926e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00165: loss improved from 0.00060 to 0.00059, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 166/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.8083e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00166: loss improved from 0.00059 to 0.00058, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 167/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.7247e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00167: loss improved from 0.00058 to 0.00057, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 168/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.6419e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00168: loss improved from 0.00057 to 0.00056, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 169/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.5601e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00169: loss improved from 0.00056 to 0.00056, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 170/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.4800e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00170: loss improved from 0.00056 to 0.00055, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 171/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.4021e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00171: loss improved from 0.00055 to 0.00054, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 172/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.3271e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00172: loss improved from 0.00054 to 0.00053, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 173/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.2550e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00173: loss improved from 0.00053 to 0.00053, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 174/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.1855e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00174: loss improved from 0.00053 to 0.00052, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 175/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.1181e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00175: loss improved from 0.00052 to 0.00051, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 176/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 5.0520e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00176: loss improved from 0.00051 to 0.00051, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 177/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.9870e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00177: loss improved from 0.00051 to 0.00050, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 178/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.9232e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00178: loss improved from 0.00050 to 0.00049, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.8602e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00179: loss improved from 0.00049 to 0.00049, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 180/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.7981e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00180: loss improved from 0.00049 to 0.00048, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 181/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.7371e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00181: loss improved from 0.00048 to 0.00047, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 182/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.6774e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00182: loss improved from 0.00047 to 0.00047, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 183/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.6192e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00183: loss improved from 0.00047 to 0.00046, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 184/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.5625e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00184: loss improved from 0.00046 to 0.00046, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 185/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.5072e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00185: loss improved from 0.00046 to 0.00045, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 186/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.4532e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00186: loss improved from 0.00045 to 0.00045, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 187/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.4005e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00187: loss improved from 0.00045 to 0.00044, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 188/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.3489e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00188: loss improved from 0.00044 to 0.00043, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 189/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.2983e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00189: loss improved from 0.00043 to 0.00043, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 190/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.2484e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00190: loss improved from 0.00043 to 0.00042, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 191/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.1994e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00191: loss improved from 0.00042 to 0.00042, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 192/200\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 4.1511e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00192: loss improved from 0.00042 to 0.00042, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 193/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.1038e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00193: loss improved from 0.00042 to 0.00041, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 194/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.0574e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00194: loss improved from 0.00041 to 0.00041, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 195/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 4.0117e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00195: loss improved from 0.00041 to 0.00040, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 196/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 3.9670e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00196: loss improved from 0.00040 to 0.00040, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 197/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 3.9233e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00197: loss improved from 0.00040 to 0.00039, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 198/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 3.8803e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00198: loss improved from 0.00039 to 0.00039, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 199/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 3.8382e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00199: loss improved from 0.00039 to 0.00038, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 200/200\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 3.7968e-04 - acc: 1.0000\n",
      "\n",
      "Epoch 00200: loss improved from 0.00038 to 0.00038, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c747d30860>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 32, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 18330)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 18330, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 18330, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 18330, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 18330, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 18330, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 18330, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 18330, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 18330, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 18330, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 18330, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 18330, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 18330, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 18330, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 18330, 40)    0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 18330, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 18330, 40)    0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 18330, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 18330, 40)    0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 18330, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 18330, 40)    0           dilated_conv_512_tanh[0][0]      \n",
      "                                                                 dilated_conv_512_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 18330, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 18330, 40)    128040      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 230, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 230, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 230, 50)      200050      conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 50)        0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 50)        5050        downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 50)        0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 50)           0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 50)           0           reshape_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 566,380\n",
      "Trainable params: 566,380\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1 sample for 1 user\n",
    "# no_users = 50\n",
    "\n",
    "wnc = WaveNetClassifier((18330,), (50,), kernel_size = 2, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "50/50 [==============================] - 11s 217ms/step - loss: 3.9160 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.91604, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.9130 - acc: 0.0200\n",
      "\n",
      "Epoch 00002: loss improved from 3.91604 to 3.91301, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.9115 - acc: 0.0200\n",
      "\n",
      "Epoch 00003: loss improved from 3.91301 to 3.91152, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.9144 - acc: 0.0200\n",
      "\n",
      "Epoch 00004: loss did not improve from 3.91152\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.8914 - acc: 0.0200\n",
      "\n",
      "Epoch 00005: loss improved from 3.91152 to 3.89143, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.9725 - acc: 0.0200\n",
      "\n",
      "Epoch 00006: loss did not improve from 3.89143\n",
      "Epoch 7/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.8183 - acc: 0.0400\n",
      "\n",
      "Epoch 00007: loss improved from 3.89143 to 3.81830, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.7745 - acc: 0.0200\n",
      "\n",
      "Epoch 00008: loss improved from 3.81830 to 3.77447, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.7352 - acc: 0.0400\n",
      "\n",
      "Epoch 00009: loss improved from 3.77447 to 3.73517, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.7043 - acc: 0.0400\n",
      "\n",
      "Epoch 00010: loss improved from 3.73517 to 3.70435, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.6436 - acc: 0.0400\n",
      "\n",
      "Epoch 00011: loss improved from 3.70435 to 3.64364, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.6229 - acc: 0.0600\n",
      "\n",
      "Epoch 00012: loss improved from 3.64364 to 3.62291, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.5543 - acc: 0.0800\n",
      "\n",
      "Epoch 00013: loss improved from 3.62291 to 3.55428, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.5752 - acc: 0.0400\n",
      "\n",
      "Epoch 00014: loss did not improve from 3.55428\n",
      "Epoch 15/200\n",
      "50/50 [==============================] - 3s 56ms/step - loss: 3.5240 - acc: 0.1000\n",
      "\n",
      "Epoch 00015: loss improved from 3.55428 to 3.52397, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 16/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 3.4825 - acc: 0.1000\n",
      "\n",
      "Epoch 00016: loss improved from 3.52397 to 3.48245, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 3.4351 - acc: 0.0800\n",
      "\n",
      "Epoch 00017: loss improved from 3.48245 to 3.43508, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - 3s 56ms/step - loss: 3.4029 - acc: 0.1200\n",
      "\n",
      "Epoch 00018: loss improved from 3.43508 to 3.40292, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.4055 - acc: 0.1000\n",
      "\n",
      "Epoch 00019: loss did not improve from 3.40292\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.3745 - acc: 0.1000\n",
      "\n",
      "Epoch 00020: loss improved from 3.40292 to 3.37450, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 21/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.2976 - acc: 0.1200\n",
      "\n",
      "Epoch 00021: loss improved from 3.37450 to 3.29762, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.1985 - acc: 0.1400\n",
      "\n",
      "Epoch 00022: loss improved from 3.29762 to 3.19855, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 23/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.1585 - acc: 0.1400\n",
      "\n",
      "Epoch 00023: loss improved from 3.19855 to 3.15852, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 3.0675 - acc: 0.1800\n",
      "\n",
      "Epoch 00024: loss improved from 3.15852 to 3.06751, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 25/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 2.9790 - acc: 0.2200\n",
      "\n",
      "Epoch 00025: loss improved from 3.06751 to 2.97899, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 2.9238 - acc: 0.2200\n",
      "\n",
      "Epoch 00026: loss improved from 2.97899 to 2.92382, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 2.9011 - acc: 0.2000\n",
      "\n",
      "Epoch 00027: loss improved from 2.92382 to 2.90107, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 2.8993 - acc: 0.2200\n",
      "\n",
      "Epoch 00028: loss improved from 2.90107 to 2.89925, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 2.7432 - acc: 0.2800\n",
      "\n",
      "Epoch 00029: loss improved from 2.89925 to 2.74319, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 2.8617 - acc: 0.2200\n",
      "\n",
      "Epoch 00030: loss did not improve from 2.74319\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 2.8214 - acc: 0.2600\n",
      "\n",
      "Epoch 00031: loss did not improve from 2.74319\n",
      "Epoch 32/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 2.6268 - acc: 0.3200\n",
      "\n",
      "Epoch 00032: loss improved from 2.74319 to 2.62678, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 33/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 2.6572 - acc: 0.3200\n",
      "\n",
      "Epoch 00033: loss did not improve from 2.62678\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.7928 - acc: 0.3200\n",
      "\n",
      "Epoch 00034: loss did not improve from 2.62678\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.9822 - acc: 0.2600\n",
      "\n",
      "Epoch 00035: loss did not improve from 2.62678\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.9196 - acc: 0.2600\n",
      "\n",
      "Epoch 00036: loss did not improve from 2.62678\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.7567 - acc: 0.3400\n",
      "\n",
      "Epoch 00037: loss did not improve from 2.62678\n",
      "Epoch 38/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.6367 - acc: 0.3200\n",
      "\n",
      "Epoch 00038: loss did not improve from 2.62678\n",
      "Epoch 39/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.4033 - acc: 0.3200\n",
      "\n",
      "Epoch 00039: loss improved from 2.62678 to 2.40334, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.4295 - acc: 0.3600\n",
      "\n",
      "Epoch 00040: loss did not improve from 2.40334\n",
      "Epoch 41/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.1824 - acc: 0.3600\n",
      "\n",
      "Epoch 00041: loss improved from 2.40334 to 2.18236, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 42/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.0608 - acc: 0.3800\n",
      "\n",
      "Epoch 00042: loss improved from 2.18236 to 2.06084, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 43/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.0158 - acc: 0.4400\n",
      "\n",
      "Epoch 00043: loss improved from 2.06084 to 2.01575, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 44/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.7547 - acc: 0.5200\n",
      "\n",
      "Epoch 00044: loss improved from 2.01575 to 1.75468, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.5761 - acc: 0.5800\n",
      "\n",
      "Epoch 00045: loss improved from 1.75468 to 1.57615, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.5141 - acc: 0.6000\n",
      "\n",
      "Epoch 00046: loss improved from 1.57615 to 1.51405, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 47/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.3515 - acc: 0.6000\n",
      "\n",
      "Epoch 00047: loss improved from 1.51405 to 1.35154, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 48/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.2754 - acc: 0.6200\n",
      "\n",
      "Epoch 00048: loss improved from 1.35154 to 1.27537, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 49/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.2700 - acc: 0.6800\n",
      "\n",
      "Epoch 00049: loss improved from 1.27537 to 1.27005, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 50/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.2258 - acc: 0.6400\n",
      "\n",
      "Epoch 00050: loss improved from 1.27005 to 1.22584, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.0075 - acc: 0.6800\n",
      "\n",
      "Epoch 00051: loss improved from 1.22584 to 1.00750, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.7218 - acc: 0.6200\n",
      "\n",
      "Epoch 00052: loss did not improve from 1.00750\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.0876 - acc: 0.3400\n",
      "\n",
      "Epoch 00053: loss did not improve from 1.00750\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.7716 - acc: 0.3600\n",
      "\n",
      "Epoch 00054: loss did not improve from 1.00750\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.0897 - acc: 0.2800\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.00750\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 3.1495 - acc: 0.3200\n",
      "\n",
      "Epoch 00056: loss did not improve from 1.00750\n",
      "Epoch 57/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.8828 - acc: 0.3600\n",
      "\n",
      "Epoch 00057: loss did not improve from 1.00750\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.4660 - acc: 0.3400\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.00750\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.2314 - acc: 0.5000\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.00750\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 2.2359 - acc: 0.4800\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.00750\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.7422 - acc: 0.6200\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.00750\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.8208 - acc: 0.6200\n",
      "\n",
      "Epoch 00062: loss did not improve from 1.00750\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.4545 - acc: 0.6600\n",
      "\n",
      "Epoch 00063: loss did not improve from 1.00750\n",
      "Epoch 64/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.3465 - acc: 0.7000\n",
      "\n",
      "Epoch 00064: loss did not improve from 1.00750\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.3463 - acc: 0.7600\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.00750\n",
      "Epoch 66/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.1800 - acc: 0.7600\n",
      "\n",
      "Epoch 00066: loss did not improve from 1.00750\n",
      "Epoch 67/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.3696 - acc: 0.8000\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.00750\n",
      "Epoch 68/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.2454 - acc: 0.7800\n",
      "\n",
      "Epoch 00068: loss did not improve from 1.00750\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.1063 - acc: 0.7600\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.00750\n",
      "Epoch 70/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.0888 - acc: 0.8000\n",
      "\n",
      "Epoch 00070: loss did not improve from 1.00750\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 1.0183 - acc: 0.8200\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.00750\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.9944 - acc: 0.8200\n",
      "\n",
      "Epoch 00072: loss improved from 1.00750 to 0.99440, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 73/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.8235 - acc: 0.9000\n",
      "\n",
      "Epoch 00073: loss improved from 0.99440 to 0.82351, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 74/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.8398 - acc: 0.8800\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.82351\n",
      "Epoch 75/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.8020 - acc: 0.9000\n",
      "\n",
      "Epoch 00075: loss improved from 0.82351 to 0.80204, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.7627 - acc: 0.9200\n",
      "\n",
      "Epoch 00076: loss improved from 0.80204 to 0.76266, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.7259 - acc: 0.9400\n",
      "\n",
      "Epoch 00077: loss improved from 0.76266 to 0.72595, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.7386 - acc: 0.9400\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.72595\n",
      "Epoch 79/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.6932 - acc: 0.9600\n",
      "\n",
      "Epoch 00079: loss improved from 0.72595 to 0.69322, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 80/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.6945 - acc: 0.9400\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.69322\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.6709 - acc: 0.9600\n",
      "\n",
      "Epoch 00081: loss improved from 0.69322 to 0.67087, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.7034 - acc: 0.9400\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.67087\n",
      "Epoch 83/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.6600 - acc: 0.9600\n",
      "\n",
      "Epoch 00083: loss improved from 0.67087 to 0.65995, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - 3s 58ms/step - loss: 0.6563 - acc: 0.9600\n",
      "\n",
      "Epoch 00084: loss improved from 0.65995 to 0.65627, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 85/200\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 0.6533 - acc: 0.9600\n",
      "\n",
      "Epoch 00085: loss improved from 0.65627 to 0.65326, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 86/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6541 - acc: 0.9600\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.65326\n",
      "Epoch 87/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6491 - acc: 0.9600\n",
      "\n",
      "Epoch 00087: loss improved from 0.65326 to 0.64912, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 88/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6480 - acc: 0.9600\n",
      "\n",
      "Epoch 00088: loss improved from 0.64912 to 0.64804, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 89/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6479 - acc: 0.9600\n",
      "\n",
      "Epoch 00089: loss improved from 0.64804 to 0.64794, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 90/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6471 - acc: 0.9600\n",
      "\n",
      "Epoch 00090: loss improved from 0.64794 to 0.64715, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6468 - acc: 0.9600\n",
      "\n",
      "Epoch 00091: loss improved from 0.64715 to 0.64681, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 92/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6463 - acc: 0.9600\n",
      "\n",
      "Epoch 00092: loss improved from 0.64681 to 0.64633, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 93/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6460 - acc: 0.9600\n",
      "\n",
      "Epoch 00093: loss improved from 0.64633 to 0.64604, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200\n",
      "50/50 [==============================] - 3s 56ms/step - loss: 0.6458 - acc: 0.9600\n",
      "\n",
      "Epoch 00094: loss improved from 0.64604 to 0.64580, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 95/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6457 - acc: 0.9600\n",
      "\n",
      "Epoch 00095: loss improved from 0.64580 to 0.64568, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 96/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6456 - acc: 0.9600\n",
      "\n",
      "Epoch 00096: loss improved from 0.64568 to 0.64557, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 97/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6455 - acc: 0.9600\n",
      "\n",
      "Epoch 00097: loss improved from 0.64557 to 0.64550, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 98/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6454 - acc: 0.9600\n",
      "\n",
      "Epoch 00098: loss improved from 0.64550 to 0.64544, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 99/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6454 - acc: 0.9600\n",
      "\n",
      "Epoch 00099: loss improved from 0.64544 to 0.64540, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 100/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6454 - acc: 0.9600\n",
      "\n",
      "Epoch 00100: loss improved from 0.64540 to 0.64536, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 0.6453 - acc: 0.9600\n",
      "\n",
      "Epoch 00101: loss improved from 0.64536 to 0.64532, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 102/200\n",
      "20/50 [===========>..................] - ETA: 1s - loss: 0.8063 - acc: 0.9500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-c9bdbc4c683c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 20, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 18330)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 18330, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 18330, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 18330, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 18330, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 18330, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 18330, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 18330, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 18330, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 18330, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 18330, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 18330, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 18330, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 18330, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 18330, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 18330, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 18330, 40)    0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 18330, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 18330, 40)    0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 18330, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 18330, 40)    0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 18330, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_tanh (Conv1D)  (None, 18330, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_sigm (Conv1D)  (None, 18330, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 18330, 40)    0           dilated_conv_512_tanh[0][0]      \n",
      "                                                                 dilated_conv_512_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 18330, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 18330, 40)    128040      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 230, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 230, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 230, 100)     400100      conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 100)       0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 100)       20100       downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 100)       0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 100)          0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 100)          0           reshape_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 781,480\n",
      "Trainable params: 781,480\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1 sample for 1 user\n",
    "# no_users = 100\n",
    "\n",
    "wnc = WaveNetClassifier((18330,), (100,), kernel_size = 2, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 4.6101 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.61014, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6058 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00002: loss improved from 4.61014 to 4.60575, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6055 - acc: 0.0100\n",
      "\n",
      "Epoch 00003: loss improved from 4.60575 to 4.60550, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00004: loss improved from 4.60550 to 4.60543, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00005: loss improved from 4.60543 to 4.60542, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00006: loss improved from 4.60542 to 4.60540, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00007: loss improved from 4.60540 to 4.60539, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00008: loss did not improve from 4.60539\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00009: loss improved from 4.60539 to 4.60538, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00010: loss did not improve from 4.60538\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00011: loss did not improve from 4.60538\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00012: loss improved from 4.60538 to 4.60538, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00013: loss did not improve from 4.60538\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00014: loss did not improve from 4.60538\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00015: loss did not improve from 4.60538\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00016: loss did not improve from 4.60538\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00017: loss improved from 4.60538 to 4.60537, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00018: loss improved from 4.60537 to 4.60537, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00019: loss did not improve from 4.60537\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00020: loss did not improve from 4.60537\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00021: loss improved from 4.60537 to 4.60537, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00022: loss did not improve from 4.60537\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00023: loss improved from 4.60537 to 4.60536, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00024: loss did not improve from 4.60536\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00025: loss did not improve from 4.60536\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00026: loss did not improve from 4.60536\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00027: loss did not improve from 4.60536\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00028: loss did not improve from 4.60536\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00029: loss did not improve from 4.60536\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00030: loss did not improve from 4.60536\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00031: loss did not improve from 4.60536\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00032: loss did not improve from 4.60536\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00033: loss did not improve from 4.60536\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00034: loss did not improve from 4.60536\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00035: loss did not improve from 4.60536\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00036: loss did not improve from 4.60536\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00037: loss improved from 4.60536 to 4.60536, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00038: loss improved from 4.60536 to 4.60536, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00039: loss did not improve from 4.60536\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00040: loss did not improve from 4.60536\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00041: loss did not improve from 4.60536\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00042: loss did not improve from 4.60536\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00043: loss did not improve from 4.60536\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00044: loss did not improve from 4.60536\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00045: loss did not improve from 4.60536\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00046: loss did not improve from 4.60536\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00047: loss did not improve from 4.60536\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00048: loss did not improve from 4.60536\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00049: loss did not improve from 4.60536\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00050: loss did not improve from 4.60536\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00051: loss did not improve from 4.60536\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00052: loss did not improve from 4.60536\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00053: loss did not improve from 4.60536\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00054: loss did not improve from 4.60536\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00055: loss did not improve from 4.60536\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00056: loss improved from 4.60536 to 4.60535, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00057: loss did not improve from 4.60535\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00058: loss did not improve from 4.60535\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00059: loss did not improve from 4.60535\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00060: loss did not improve from 4.60535\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00061: loss did not improve from 4.60535\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00062: loss did not improve from 4.60535\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00063: loss did not improve from 4.60535\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00064: loss improved from 4.60535 to 4.60535, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00065: loss did not improve from 4.60535\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00066: loss did not improve from 4.60535\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00067: loss did not improve from 4.60535\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00068: loss did not improve from 4.60535\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00069: loss did not improve from 4.60535\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00070: loss did not improve from 4.60535\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00071: loss improved from 4.60535 to 4.60535, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00072: loss did not improve from 4.60535\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00073: loss did not improve from 4.60535\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00074: loss did not improve from 4.60535\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00075: loss did not improve from 4.60535\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00076: loss did not improve from 4.60535\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00077: loss did not improve from 4.60535\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00078: loss did not improve from 4.60535\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00079: loss did not improve from 4.60535\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00080: loss did not improve from 4.60535\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00081: loss did not improve from 4.60535\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00082: loss did not improve from 4.60535\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6053 - acc: 0.0100\n",
      "\n",
      "Epoch 00083: loss improved from 4.60535 to 4.60535, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00084: loss did not improve from 4.60535\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00085: loss did not improve from 4.60535\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00086: loss did not improve from 4.60535\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00087: loss did not improve from 4.60535\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00088: loss did not improve from 4.60535\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00089: loss did not improve from 4.60535\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00090: loss did not improve from 4.60535\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00091: loss did not improve from 4.60535\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.6053 - acc: 0.0100\n",
      "\n",
      "Epoch 00092: loss did not improve from 4.60535\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00093: loss did not improve from 4.60535\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00094: loss did not improve from 4.60535\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00095: loss did not improve from 4.60535\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00096: loss did not improve from 4.60535\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00097: loss did not improve from 4.60535\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0100\n",
      "\n",
      "Epoch 00098: loss did not improve from 4.60535\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 4.6054 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00099: loss did not improve from 4.60535\n",
      "Epoch 100/200\n",
      " 40/100 [===========>..................] - ETA: 3s - loss: 4.6052 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-c9bdbc4c683c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 20, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 23962)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 23962, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 23962, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 23962, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 23962, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 23962, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 23962, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 23962, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 23962, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 23962, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 23962, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 23962, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 23962, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 23962, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 23962, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 23962, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 23962, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 23962, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 23962, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 23962, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 23962, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 23962, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 23962, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 23962, 40)    3240        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 23962, 40)    0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 23962, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 23962, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 23962, 40)    3240        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 23962, 40)    0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 23962, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 23962, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 23962, 40)    3240        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 23962, 40)    0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 23962, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_tanh (Conv1D)  (None, 23962, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_512_sigm (Conv1D)  (None, 23962, 40)    3240        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 23962, 40)    0           dilated_conv_512_tanh[0][0]      \n",
      "                                                                 dilated_conv_512_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 23962, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 23962, 40)    128040      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 300, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 300, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 300, 10)      40010       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 10)        0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 10)        210         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 10)        0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 10)           0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 10)           0           reshape_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 401,500\n",
      "Trainable params: 401,500\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2 samples for 1 user\n",
    "# no_users = 10\n",
    "\n",
    "wnc = WaveNetClassifier((23962,), (10,), kernel_size = 2, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20/20 [==============================] - 12s 617ms/step - loss: 2.3040 - acc: 0.1000\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.30398, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3121 - acc: 0.1000\n",
      "\n",
      "Epoch 00002: loss did not improve from 2.30398\n",
      "Epoch 3/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3032 - acc: 0.1000\n",
      "\n",
      "Epoch 00003: loss improved from 2.30398 to 2.30319, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00004: loss improved from 2.30319 to 2.30275, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 5/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3028 - acc: 0.1000\n",
      "\n",
      "Epoch 00005: loss did not improve from 2.30275\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00006: loss improved from 2.30275 to 2.30262, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 7/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00007: loss improved from 2.30262 to 2.30258, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00008: loss did not improve from 2.30258\n",
      "Epoch 9/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00009: loss improved from 2.30258 to 2.30257, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 10/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3025 - acc: 0.1000\n",
      "\n",
      "Epoch 00010: loss improved from 2.30257 to 2.30255, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 11/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00011: loss did not improve from 2.30255\n",
      "Epoch 12/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00012: loss did not improve from 2.30255\n",
      "Epoch 13/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00013: loss did not improve from 2.30255\n",
      "Epoch 14/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00014: loss did not improve from 2.30255\n",
      "Epoch 15/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00015: loss did not improve from 2.30255\n",
      "Epoch 16/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.30255\n",
      "Epoch 17/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00017: loss did not improve from 2.30255\n",
      "Epoch 18/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00018: loss did not improve from 2.30255\n",
      "Epoch 19/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00019: loss did not improve from 2.30255\n",
      "Epoch 20/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00020: loss did not improve from 2.30255\n",
      "Epoch 21/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00021: loss did not improve from 2.30255\n",
      "Epoch 22/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00022: loss did not improve from 2.30255\n",
      "Epoch 23/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00023: loss did not improve from 2.30255\n",
      "Epoch 24/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00024: loss did not improve from 2.30255\n",
      "Epoch 25/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00025: loss did not improve from 2.30255\n",
      "Epoch 26/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00026: loss did not improve from 2.30255\n",
      "Epoch 27/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00027: loss did not improve from 2.30255\n",
      "Epoch 28/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00028: loss did not improve from 2.30255\n",
      "Epoch 29/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00029: loss did not improve from 2.30255\n",
      "Epoch 30/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00030: loss did not improve from 2.30255\n",
      "Epoch 31/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00031: loss did not improve from 2.30255\n",
      "Epoch 32/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00032: loss did not improve from 2.30255\n",
      "Epoch 33/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00033: loss did not improve from 2.30255\n",
      "Epoch 34/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00034: loss did not improve from 2.30255\n",
      "Epoch 35/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00035: loss did not improve from 2.30255\n",
      "Epoch 36/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00036: loss did not improve from 2.30255\n",
      "Epoch 37/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00037: loss did not improve from 2.30255\n",
      "Epoch 38/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00038: loss did not improve from 2.30255\n",
      "Epoch 39/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00039: loss did not improve from 2.30255\n",
      "Epoch 40/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00040: loss did not improve from 2.30255\n",
      "Epoch 41/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00041: loss did not improve from 2.30255\n",
      "Epoch 42/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00042: loss did not improve from 2.30255\n",
      "Epoch 43/200\n",
      "20/20 [==============================] - 2s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00043: loss did not improve from 2.30255\n",
      "Epoch 44/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00044: loss did not improve from 2.30255\n",
      "Epoch 45/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00045: loss did not improve from 2.30255\n",
      "Epoch 46/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00046: loss did not improve from 2.30255\n",
      "Epoch 47/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00047: loss did not improve from 2.30255\n",
      "Epoch 48/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00048: loss did not improve from 2.30255\n",
      "Epoch 49/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00049: loss did not improve from 2.30255\n",
      "Epoch 50/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00050: loss did not improve from 2.30255\n",
      "Epoch 51/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00051: loss did not improve from 2.30255\n",
      "Epoch 52/200\n",
      "20/20 [==============================] - 2s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00052: loss did not improve from 2.30255\n",
      "Epoch 53/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00053: loss did not improve from 2.30255\n",
      "Epoch 54/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00054: loss did not improve from 2.30255\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00055: loss did not improve from 2.30255\n",
      "Epoch 56/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00056: loss did not improve from 2.30255\n",
      "Epoch 57/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00057: loss did not improve from 2.30255\n",
      "Epoch 58/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00058: loss did not improve from 2.30255\n",
      "Epoch 59/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00059: loss did not improve from 2.30255\n",
      "Epoch 60/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00060: loss did not improve from 2.30255\n",
      "Epoch 61/200\n",
      "20/20 [==============================] - 2s 78ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00061: loss did not improve from 2.30255\n",
      "Epoch 62/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00062: loss did not improve from 2.30255\n",
      "Epoch 63/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00063: loss did not improve from 2.30255\n",
      "Epoch 64/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00064: loss did not improve from 2.30255\n",
      "Epoch 65/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00065: loss did not improve from 2.30255\n",
      "Epoch 66/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00066: loss did not improve from 2.30255\n",
      "Epoch 67/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00067: loss did not improve from 2.30255\n",
      "Epoch 68/200\n",
      "20/20 [==============================] - 2s 78ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00068: loss did not improve from 2.30255\n",
      "Epoch 69/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00069: loss did not improve from 2.30255\n",
      "Epoch 70/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00070: loss did not improve from 2.30255\n",
      "Epoch 71/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00071: loss did not improve from 2.30255\n",
      "Epoch 72/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00072: loss did not improve from 2.30255\n",
      "Epoch 73/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00073: loss did not improve from 2.30255\n",
      "Epoch 74/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00074: loss did not improve from 2.30255\n",
      "Epoch 75/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00075: loss did not improve from 2.30255\n",
      "Epoch 76/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00076: loss did not improve from 2.30255\n",
      "Epoch 77/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00077: loss did not improve from 2.30255\n",
      "Epoch 78/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00078: loss did not improve from 2.30255\n",
      "Epoch 79/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00079: loss did not improve from 2.30255\n",
      "Epoch 80/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00080: loss did not improve from 2.30255\n",
      "Epoch 81/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00081: loss did not improve from 2.30255\n",
      "Epoch 82/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00082: loss did not improve from 2.30255\n",
      "Epoch 83/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00083: loss did not improve from 2.30255\n",
      "Epoch 84/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00084: loss did not improve from 2.30255\n",
      "Epoch 85/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00085: loss did not improve from 2.30255\n",
      "Epoch 86/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00086: loss did not improve from 2.30255\n",
      "Epoch 87/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00087: loss did not improve from 2.30255\n",
      "Epoch 88/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00088: loss did not improve from 2.30255\n",
      "Epoch 89/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00089: loss did not improve from 2.30255\n",
      "Epoch 90/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00090: loss did not improve from 2.30255\n",
      "Epoch 91/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00091: loss did not improve from 2.30255\n",
      "Epoch 92/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00092: loss did not improve from 2.30255\n",
      "Epoch 93/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00093: loss did not improve from 2.30255\n",
      "Epoch 94/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00094: loss did not improve from 2.30255\n",
      "Epoch 95/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00095: loss did not improve from 2.30255\n",
      "Epoch 96/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00096: loss did not improve from 2.30255\n",
      "Epoch 97/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00097: loss did not improve from 2.30255\n",
      "Epoch 98/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00098: loss did not improve from 2.30255\n",
      "Epoch 99/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00099: loss did not improve from 2.30255\n",
      "Epoch 100/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00100: loss did not improve from 2.30255\n",
      "Epoch 101/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00101: loss did not improve from 2.30255\n",
      "Epoch 102/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00102: loss did not improve from 2.30255\n",
      "Epoch 103/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00103: loss did not improve from 2.30255\n",
      "Epoch 104/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00104: loss did not improve from 2.30255\n",
      "Epoch 105/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00105: loss did not improve from 2.30255\n",
      "Epoch 106/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00106: loss did not improve from 2.30255\n",
      "Epoch 107/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00107: loss did not improve from 2.30255\n",
      "Epoch 108/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00108: loss did not improve from 2.30255\n",
      "Epoch 109/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00109: loss did not improve from 2.30255\n",
      "Epoch 110/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00110: loss did not improve from 2.30255\n",
      "Epoch 111/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00111: loss did not improve from 2.30255\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00112: loss did not improve from 2.30255\n",
      "Epoch 113/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00113: loss did not improve from 2.30255\n",
      "Epoch 114/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00114: loss did not improve from 2.30255\n",
      "Epoch 115/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00115: loss did not improve from 2.30255\n",
      "Epoch 116/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00116: loss did not improve from 2.30255\n",
      "Epoch 117/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00117: loss did not improve from 2.30255\n",
      "Epoch 118/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00118: loss did not improve from 2.30255\n",
      "Epoch 119/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00119: loss did not improve from 2.30255\n",
      "Epoch 120/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00120: loss did not improve from 2.30255\n",
      "Epoch 121/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00121: loss did not improve from 2.30255\n",
      "Epoch 122/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00122: loss did not improve from 2.30255\n",
      "Epoch 123/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00123: loss did not improve from 2.30255\n",
      "Epoch 124/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00124: loss did not improve from 2.30255\n",
      "Epoch 125/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00125: loss did not improve from 2.30255\n",
      "Epoch 126/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00126: loss did not improve from 2.30255\n",
      "Epoch 127/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00127: loss did not improve from 2.30255\n",
      "Epoch 128/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00128: loss did not improve from 2.30255\n",
      "Epoch 129/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00129: loss did not improve from 2.30255\n",
      "Epoch 130/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00130: loss did not improve from 2.30255\n",
      "Epoch 131/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00131: loss did not improve from 2.30255\n",
      "Epoch 132/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00132: loss did not improve from 2.30255\n",
      "Epoch 133/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00133: loss did not improve from 2.30255\n",
      "Epoch 134/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00134: loss did not improve from 2.30255\n",
      "Epoch 135/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00135: loss did not improve from 2.30255\n",
      "Epoch 136/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00136: loss did not improve from 2.30255\n",
      "Epoch 137/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00137: loss did not improve from 2.30255\n",
      "Epoch 138/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00138: loss did not improve from 2.30255\n",
      "Epoch 139/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00139: loss did not improve from 2.30255\n",
      "Epoch 140/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00140: loss did not improve from 2.30255\n",
      "Epoch 141/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00141: loss did not improve from 2.30255\n",
      "Epoch 142/200\n",
      "20/20 [==============================] - 2s 78ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00142: loss did not improve from 2.30255\n",
      "Epoch 143/200\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00143: loss did not improve from 2.30255\n",
      "Epoch 144/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00144: loss did not improve from 2.30255\n",
      "Epoch 145/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00145: loss did not improve from 2.30255\n",
      "Epoch 146/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00146: loss did not improve from 2.30255\n",
      "Epoch 147/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00147: loss did not improve from 2.30255\n",
      "Epoch 148/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00148: loss did not improve from 2.30255\n",
      "Epoch 149/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00149: loss did not improve from 2.30255\n",
      "Epoch 150/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00150: loss did not improve from 2.30255\n",
      "Epoch 151/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00151: loss did not improve from 2.30255\n",
      "Epoch 152/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00152: loss did not improve from 2.30255\n",
      "Epoch 153/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00153: loss did not improve from 2.30255\n",
      "Epoch 154/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00154: loss did not improve from 2.30255\n",
      "Epoch 155/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00155: loss did not improve from 2.30255\n",
      "Epoch 156/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00156: loss did not improve from 2.30255\n",
      "Epoch 157/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00157: loss did not improve from 2.30255\n",
      "Epoch 158/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00158: loss did not improve from 2.30255\n",
      "Epoch 159/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00159: loss did not improve from 2.30255\n",
      "Epoch 160/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00160: loss did not improve from 2.30255\n",
      "Epoch 161/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00161: loss did not improve from 2.30255\n",
      "Epoch 162/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00162: loss did not improve from 2.30255\n",
      "Epoch 163/200\n",
      "20/20 [==============================] - 2s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00163: loss did not improve from 2.30255\n",
      "Epoch 164/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00164: loss did not improve from 2.30255\n",
      "Epoch 165/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00165: loss did not improve from 2.30255\n",
      "Epoch 166/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00166: loss did not improve from 2.30255\n",
      "Epoch 167/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00167: loss did not improve from 2.30255\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00168: loss did not improve from 2.30255\n",
      "Epoch 169/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00169: loss did not improve from 2.30255\n",
      "Epoch 170/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00170: loss did not improve from 2.30255\n",
      "Epoch 171/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00171: loss did not improve from 2.30255\n",
      "Epoch 172/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00172: loss did not improve from 2.30255\n",
      "Epoch 173/200\n",
      "20/20 [==============================] - 2s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00173: loss did not improve from 2.30255\n",
      "Epoch 174/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00174: loss did not improve from 2.30255\n",
      "Epoch 175/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00175: loss did not improve from 2.30255\n",
      "Epoch 176/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00176: loss did not improve from 2.30255\n",
      "Epoch 177/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00177: loss did not improve from 2.30255\n",
      "Epoch 178/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00178: loss did not improve from 2.30255\n",
      "Epoch 179/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00179: loss did not improve from 2.30255\n",
      "Epoch 180/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00180: loss did not improve from 2.30255\n",
      "Epoch 181/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00181: loss did not improve from 2.30255\n",
      "Epoch 182/200\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00182: loss did not improve from 2.30255\n",
      "Epoch 183/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00183: loss did not improve from 2.30255\n",
      "Epoch 184/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00184: loss did not improve from 2.30255\n",
      "Epoch 185/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00185: loss did not improve from 2.30255\n",
      "Epoch 186/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00186: loss did not improve from 2.30255\n",
      "Epoch 187/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00187: loss did not improve from 2.30255\n",
      "Epoch 188/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00188: loss did not improve from 2.30255\n",
      "Epoch 189/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00189: loss did not improve from 2.30255\n",
      "Epoch 190/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00190: loss did not improve from 2.30255\n",
      "Epoch 191/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00191: loss did not improve from 2.30255\n",
      "Epoch 192/200\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00192: loss did not improve from 2.30255\n",
      "Epoch 193/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00193: loss did not improve from 2.30255\n",
      "Epoch 194/200\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00194: loss did not improve from 2.30255\n",
      "Epoch 195/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00195: loss did not improve from 2.30255\n",
      "Epoch 196/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00196: loss did not improve from 2.30255\n",
      "Epoch 197/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00197: loss did not improve from 2.30255\n",
      "Epoch 198/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00198: loss did not improve from 2.30255\n",
      "Epoch 199/200\n",
      "20/20 [==============================] - 2s 76ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00199: loss did not improve from 2.30255\n",
      "Epoch 200/200\n",
      "20/20 [==============================] - 1s 75ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00200: loss did not improve from 2.30255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c7d2dc5d30>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 16, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 23962)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 23962, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 23962, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_5_tanh (Conv1D)    (None, 23962, 40)    8040        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_5_sigm (Conv1D)    (None, 23962, 40)    8040        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 23962, 40)    0           dilated_conv_5_tanh[0][0]        \n",
      "                                                                 dilated_conv_5_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_25_tanh (Conv1D)   (None, 23962, 40)    8040        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_25_sigm (Conv1D)   (None, 23962, 40)    8040        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 23962, 40)    0           dilated_conv_25_tanh[0][0]       \n",
      "                                                                 dilated_conv_25_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 23962, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_125_tanh (Conv1D)  (None, 23962, 40)    8040        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_125_sigm (Conv1D)  (None, 23962, 40)    8040        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 23962, 40)    0           dilated_conv_125_tanh[0][0]      \n",
      "                                                                 dilated_conv_125_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 23962, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_625_tanh (Conv1D)  (None, 23962, 40)    8040        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_625_sigm (Conv1D)  (None, 23962, 40)    8040        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 23962, 40)    0           dilated_conv_625_tanh[0][0]      \n",
      "                                                                 dilated_conv_625_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 23962, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_3125_tanh (Conv1D) (None, 23962, 40)    8040        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_3125_sigm (Conv1D) (None, 23962, 40)    8040        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 23962, 40)    0           dilated_conv_3125_tanh[0][0]     \n",
      "                                                                 dilated_conv_3125_sigm[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 23962, 40)    0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_15625_tanh (Conv1D (None, 23962, 40)    8040        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_15625_sigm (Conv1D (None, 23962, 40)    8040        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 23962, 40)    0           dilated_conv_15625_tanh[0][0]    \n",
      "                                                                 dilated_conv_15625_sigm[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 23962, 40)    0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_78125_tanh (Conv1D (None, 23962, 40)    8040        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_78125_sigm (Conv1D (None, 23962, 40)    8040        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 23962, 40)    0           dilated_conv_78125_tanh[0][0]    \n",
      "                                                                 dilated_conv_78125_sigm[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 23962, 40)    0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_390625_tanh (Conv1 (None, 23962, 40)    8040        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_390625_sigm (Conv1 (None, 23962, 40)    8040        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 23962, 40)    0           dilated_conv_390625_tanh[0][0]   \n",
      "                                                                 dilated_conv_390625_sigm[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_8 (Add)          (None, 23962, 40)    0           skip_8[0][0]                     \n",
      "                                                                 residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1953125_tanh (Conv (None, 23962, 40)    8040        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1953125_sigm (Conv (None, 23962, 40)    8040        residual_block_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_9 (Multiply)   (None, 23962, 40)    0           dilated_conv_1953125_tanh[0][0]  \n",
      "                                                                 dilated_conv_1953125_sigm[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "skip_9 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "                                                                 skip_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 23962, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 23962, 40)    128040      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 300, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 300, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 300, 10)      40010       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 10)        0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 10)        210         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 10)        0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 10)           0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 10)           0           reshape_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 487,900\n",
      "Trainable params: 487,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # 2 samples for 1 user\n",
    "# no_users = 10\n",
    "# kernel_size = 5\n",
    "\n",
    "wnc = WaveNetClassifier((23962,), (10,), kernel_size = 5, dilation_depth = 9, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1953125,5,40] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node dilated_conv_1953125_tanh/convolution/SpaceToBatchND}} = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _class=[\"loc:@train...hToSpaceND\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dilated_conv_1953125_tanh/Pad, dilated_conv_1953125_tanh/convolution/SpaceToBatchND/block_shape, dilated_conv_1953125_tanh/convolution/SpaceToBatchND/paddings)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node metrics_21/acc/Mean/_15275}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5362_metrics_21/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-8fd8f11d59c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    527\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1953125,5,40] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node dilated_conv_1953125_tanh/convolution/SpaceToBatchND}} = SpaceToBatchND[T=DT_FLOAT, Tblock_shape=DT_INT32, Tpaddings=DT_INT32, _class=[\"loc:@train...hToSpaceND\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dilated_conv_1953125_tanh/Pad, dilated_conv_1953125_tanh/convolution/SpaceToBatchND/block_shape, dilated_conv_1953125_tanh/convolution/SpaceToBatchND/paddings)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node metrics_21/acc/Mean/_15275}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5362_metrics_21/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 1, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 23962)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 23962, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 23962, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 23962, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 23962, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 23962, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 23962, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 23962, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 23962, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 23962, 40)    0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 23962, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 23962, 40)    3240        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 23962, 40)    0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 23962, 40)    0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 23962, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 23962, 40)    3240        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 23962, 40)    0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 23962, 40)    0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 23962, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 23962, 40)    3240        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 23962, 40)    0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 23962, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 23962, 40)    128040      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 300, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 300, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 300, 10)      40010       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 10)        0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 10)        210         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 10)        0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 10)           0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 10)           0           reshape_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 369,020\n",
      "Trainable params: 369,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # 2 samples for 1 user\n",
    "# no_users = 10\n",
    "# dilation_depth = 5\n",
    "\n",
    "wnc = WaveNetClassifier((23962,), (10,), kernel_size = 2, dilation_depth = 5, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20/20 [==============================] - 11s 531ms/step - loss: 2.3029 - acc: 0.1000\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.30286, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 2.3054 - acc: 0.0500\n",
      "\n",
      "Epoch 00002: loss did not improve from 2.30286\n",
      "Epoch 3/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.3030 - acc: 0.1000\n",
      "\n",
      "Epoch 00003: loss did not improve from 2.30286\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2996 - acc: 0.1000\n",
      "\n",
      "Epoch 00004: loss improved from 2.30286 to 2.29961, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 5/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 2.2920 - acc: 0.1000\n",
      "\n",
      "Epoch 00005: loss improved from 2.29961 to 2.29203, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2875 - acc: 0.1000\n",
      "\n",
      "Epoch 00006: loss improved from 2.29203 to 2.28750, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 7/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2686 - acc: 0.1000\n",
      "\n",
      "Epoch 00007: loss improved from 2.28750 to 2.26859, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2630 - acc: 0.1000\n",
      "\n",
      "Epoch 00008: loss improved from 2.26859 to 2.26303, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 9/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2080 - acc: 0.1000\n",
      "\n",
      "Epoch 00009: loss improved from 2.26303 to 2.20798, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 10/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 2.1680 - acc: 0.1000\n",
      "\n",
      "Epoch 00010: loss improved from 2.20798 to 2.16801, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 11/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.1594 - acc: 0.2000\n",
      "\n",
      "Epoch 00011: loss improved from 2.16801 to 2.15943, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 12/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.1992 - acc: 0.2000\n",
      "\n",
      "Epoch 00012: loss did not improve from 2.15943\n",
      "Epoch 13/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.1360 - acc: 0.2000\n",
      "\n",
      "Epoch 00013: loss improved from 2.15943 to 2.13599, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 14/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.1917 - acc: 0.2000\n",
      "\n",
      "Epoch 00014: loss did not improve from 2.13599\n",
      "Epoch 15/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 2.1887 - acc: 0.3000\n",
      "\n",
      "Epoch 00015: loss did not improve from 2.13599\n",
      "Epoch 16/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 2.1380 - acc: 0.1500\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.13599\n",
      "Epoch 17/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.0857 - acc: 0.2000\n",
      "\n",
      "Epoch 00017: loss improved from 2.13599 to 2.08570, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 18/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.0588 - acc: 0.2000\n",
      "\n",
      "Epoch 00018: loss improved from 2.08570 to 2.05880, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 19/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.0463 - acc: 0.1000\n",
      "\n",
      "Epoch 00019: loss improved from 2.05880 to 2.04625, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 20/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.0045 - acc: 0.1500\n",
      "\n",
      "Epoch 00020: loss improved from 2.04625 to 2.00447, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 21/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.0019 - acc: 0.1500\n",
      "\n",
      "Epoch 00021: loss improved from 2.00447 to 2.00192, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 22/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.0528 - acc: 0.1500\n",
      "\n",
      "Epoch 00022: loss did not improve from 2.00192\n",
      "Epoch 23/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1.9668 - acc: 0.2000\n",
      "\n",
      "Epoch 00023: loss improved from 2.00192 to 1.96676, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.1201 - acc: 0.2000\n",
      "\n",
      "Epoch 00024: loss did not improve from 1.96676\n",
      "Epoch 25/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.0781 - acc: 0.3000\n",
      "\n",
      "Epoch 00025: loss did not improve from 1.96676\n",
      "Epoch 26/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 7.6857 - acc: 0.2500\n",
      "\n",
      "Epoch 00026: loss did not improve from 1.96676\n",
      "Epoch 27/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.1928 - acc: 0.2500\n",
      "\n",
      "Epoch 00027: loss did not improve from 1.96676\n",
      "Epoch 28/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2743 - acc: 0.1000\n",
      "\n",
      "Epoch 00028: loss did not improve from 1.96676\n",
      "Epoch 29/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2879 - acc: 0.1000\n",
      "\n",
      "Epoch 00029: loss did not improve from 1.96676\n",
      "Epoch 30/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 2.2853 - acc: 0.0500\n",
      "\n",
      "Epoch 00030: loss did not improve from 1.96676\n",
      "Epoch 31/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2728 - acc: 0.1000\n",
      "\n",
      "Epoch 00031: loss did not improve from 1.96676\n",
      "Epoch 32/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2369 - acc: 0.2000\n",
      "\n",
      "Epoch 00032: loss did not improve from 1.96676\n",
      "Epoch 33/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.1490 - acc: 0.3000\n",
      "\n",
      "Epoch 00033: loss did not improve from 1.96676\n",
      "Epoch 34/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.2824 - acc: 0.1000\n",
      "\n",
      "Epoch 00034: loss did not improve from 1.96676\n",
      "Epoch 35/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 2.0092 - acc: 0.1500\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.96676\n",
      "Epoch 36/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 1.9636 - acc: 0.2500\n",
      "\n",
      "Epoch 00036: loss improved from 1.96676 to 1.96365, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 37/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1.9710 - acc: 0.2000\n",
      "\n",
      "Epoch 00037: loss did not improve from 1.96365\n",
      "Epoch 38/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.9015 - acc: 0.4000\n",
      "\n",
      "Epoch 00038: loss improved from 1.96365 to 1.90147, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 39/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 1.9748 - acc: 0.3000\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.90147\n",
      "Epoch 40/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 2.0181 - acc: 0.2500\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.90147\n",
      "Epoch 41/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 2.0106 - acc: 0.2500\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.90147\n",
      "Epoch 42/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.9549 - acc: 0.2500\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.90147\n",
      "Epoch 43/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.9040 - acc: 0.3000\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.90147\n",
      "Epoch 44/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.8554 - acc: 0.3500\n",
      "\n",
      "Epoch 00044: loss improved from 1.90147 to 1.85538, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 45/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8377 - acc: 0.3000\n",
      "\n",
      "Epoch 00045: loss improved from 1.85538 to 1.83774, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 46/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8812 - acc: 0.3000\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.83774\n",
      "Epoch 47/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.9184 - acc: 0.3000\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.83774\n",
      "Epoch 48/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.9094 - acc: 0.3000\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.83774\n",
      "Epoch 49/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8598 - acc: 0.3500\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.83774\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 50ms/step - loss: 1.7925 - acc: 0.3500\n",
      "\n",
      "Epoch 00050: loss improved from 1.83774 to 1.79248, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 51/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8704 - acc: 0.3000\n",
      "\n",
      "Epoch 00051: loss did not improve from 1.79248\n",
      "Epoch 52/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.7926 - acc: 0.3500\n",
      "\n",
      "Epoch 00052: loss did not improve from 1.79248\n",
      "Epoch 53/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8818 - acc: 0.3500\n",
      "\n",
      "Epoch 00053: loss did not improve from 1.79248\n",
      "Epoch 54/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.9484 - acc: 0.3000\n",
      "\n",
      "Epoch 00054: loss did not improve from 1.79248\n",
      "Epoch 55/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.9579 - acc: 0.3000\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.79248\n",
      "Epoch 56/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.9220 - acc: 0.3000\n",
      "\n",
      "Epoch 00056: loss did not improve from 1.79248\n",
      "Epoch 57/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8345 - acc: 0.3500\n",
      "\n",
      "Epoch 00057: loss did not improve from 1.79248\n",
      "Epoch 58/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.7521 - acc: 0.3500\n",
      "\n",
      "Epoch 00058: loss improved from 1.79248 to 1.75213, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 59/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.7287 - acc: 0.4000\n",
      "\n",
      "Epoch 00059: loss improved from 1.75213 to 1.72867, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 60/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8124 - acc: 0.3500\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.72867\n",
      "Epoch 61/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8205 - acc: 0.4000\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.72867\n",
      "Epoch 62/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.6795 - acc: 0.5000\n",
      "\n",
      "Epoch 00062: loss improved from 1.72867 to 1.67954, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 63/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.6842 - acc: 0.5500\n",
      "\n",
      "Epoch 00063: loss did not improve from 1.67954\n",
      "Epoch 64/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.6538 - acc: 0.5500\n",
      "\n",
      "Epoch 00064: loss improved from 1.67954 to 1.65378, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 65/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.6598 - acc: 0.4500\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.65378\n",
      "Epoch 66/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.6075 - acc: 0.5500\n",
      "\n",
      "Epoch 00066: loss improved from 1.65378 to 1.60753, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 67/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.6087 - acc: 0.5000\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.60753\n",
      "Epoch 68/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.5813 - acc: 0.5000\n",
      "\n",
      "Epoch 00068: loss improved from 1.60753 to 1.58130, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 69/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.5246 - acc: 0.5500\n",
      "\n",
      "Epoch 00069: loss improved from 1.58130 to 1.52459, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 70/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.5013 - acc: 0.4500\n",
      "\n",
      "Epoch 00070: loss improved from 1.52459 to 1.50128, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 71/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.4525 - acc: 0.4500\n",
      "\n",
      "Epoch 00071: loss improved from 1.50128 to 1.45250, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 72/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.4234 - acc: 0.4500\n",
      "\n",
      "Epoch 00072: loss improved from 1.45250 to 1.42342, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 73/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.3793 - acc: 0.5500\n",
      "\n",
      "Epoch 00073: loss improved from 1.42342 to 1.37933, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 74/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.3637 - acc: 0.5500\n",
      "\n",
      "Epoch 00074: loss improved from 1.37933 to 1.36372, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 75/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.3305 - acc: 0.5000\n",
      "\n",
      "Epoch 00075: loss improved from 1.36372 to 1.33045, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 76/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.3079 - acc: 0.5000\n",
      "\n",
      "Epoch 00076: loss improved from 1.33045 to 1.30787, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 77/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.2928 - acc: 0.5500\n",
      "\n",
      "Epoch 00077: loss improved from 1.30787 to 1.29283, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 78/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.3555 - acc: 0.4000\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.29283\n",
      "Epoch 79/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.2248 - acc: 0.5500\n",
      "\n",
      "Epoch 00079: loss improved from 1.29283 to 1.22481, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 80/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.2156 - acc: 0.5000\n",
      "\n",
      "Epoch 00080: loss improved from 1.22481 to 1.21560, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 81/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.2204 - acc: 0.5000\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.21560\n",
      "Epoch 82/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.1651 - acc: 0.5500\n",
      "\n",
      "Epoch 00082: loss improved from 1.21560 to 1.16506, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 83/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.1916 - acc: 0.6000\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.16506\n",
      "Epoch 84/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.1118 - acc: 0.6000\n",
      "\n",
      "Epoch 00084: loss improved from 1.16506 to 1.11178, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 85/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.1315 - acc: 0.5000\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.11178\n",
      "Epoch 86/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.0490 - acc: 0.6000\n",
      "\n",
      "Epoch 00086: loss improved from 1.11178 to 1.04900, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 87/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.0175 - acc: 0.6000\n",
      "\n",
      "Epoch 00087: loss improved from 1.04900 to 1.01750, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 88/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.0044 - acc: 0.6000\n",
      "\n",
      "Epoch 00088: loss improved from 1.01750 to 1.00439, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 89/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.9637 - acc: 0.6000\n",
      "\n",
      "Epoch 00089: loss improved from 1.00439 to 0.96373, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 90/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.9586 - acc: 0.6000\n",
      "\n",
      "Epoch 00090: loss improved from 0.96373 to 0.95857, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 91/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.9255 - acc: 0.5500\n",
      "\n",
      "Epoch 00091: loss improved from 0.95857 to 0.92552, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 92/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.9409 - acc: 0.6500\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.92552\n",
      "Epoch 93/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.8501 - acc: 0.6500\n",
      "\n",
      "Epoch 00093: loss improved from 0.92552 to 0.85013, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 94/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.9293 - acc: 0.6000\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.85013\n",
      "Epoch 95/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.8058 - acc: 0.5000\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.85013\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 50ms/step - loss: 3.2578 - acc: 0.3000\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.85013\n",
      "Epoch 97/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 3.6797 - acc: 0.2000\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.85013\n",
      "Epoch 98/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 2.5910 - acc: 0.4000\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.85013\n",
      "Epoch 99/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 2.4718 - acc: 0.4000\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.85013\n",
      "Epoch 100/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 2.3765 - acc: 0.4500\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.85013\n",
      "Epoch 101/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 2.2610 - acc: 0.4000\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.85013\n",
      "Epoch 102/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 2.4596 - acc: 0.3000\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.85013\n",
      "Epoch 103/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 2.0377 - acc: 0.4500\n",
      "\n",
      "Epoch 00103: loss did not improve from 0.85013\n",
      "Epoch 104/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.7996 - acc: 0.4000\n",
      "\n",
      "Epoch 00104: loss did not improve from 0.85013\n",
      "Epoch 105/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.7037 - acc: 0.3000\n",
      "\n",
      "Epoch 00105: loss did not improve from 0.85013\n",
      "Epoch 106/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.4712 - acc: 0.3500\n",
      "\n",
      "Epoch 00106: loss did not improve from 0.85013\n",
      "Epoch 107/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.3460 - acc: 0.5500\n",
      "\n",
      "Epoch 00107: loss did not improve from 0.85013\n",
      "Epoch 108/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.3408 - acc: 0.5000\n",
      "\n",
      "Epoch 00108: loss did not improve from 0.85013\n",
      "Epoch 109/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.3260 - acc: 0.5500\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.85013\n",
      "Epoch 110/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.2486 - acc: 0.5500\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.85013\n",
      "Epoch 111/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.2012 - acc: 0.5000\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.85013\n",
      "Epoch 112/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.1815 - acc: 0.5000\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.85013\n",
      "Epoch 113/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.1015 - acc: 0.5500\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.85013\n",
      "Epoch 114/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.1064 - acc: 0.5500\n",
      "\n",
      "Epoch 00114: loss did not improve from 0.85013\n",
      "Epoch 115/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.0701 - acc: 0.5500\n",
      "\n",
      "Epoch 00115: loss did not improve from 0.85013\n",
      "Epoch 116/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.0354 - acc: 0.5500\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.85013\n",
      "Epoch 117/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 1.0343 - acc: 0.5500\n",
      "\n",
      "Epoch 00117: loss did not improve from 0.85013\n",
      "Epoch 118/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.0002 - acc: 0.6000\n",
      "\n",
      "Epoch 00118: loss did not improve from 0.85013\n",
      "Epoch 119/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.9769 - acc: 0.6000\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.85013\n",
      "Epoch 120/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.9627 - acc: 0.6000\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.85013\n",
      "Epoch 121/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.9307 - acc: 0.6000\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.85013\n",
      "Epoch 122/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.9017 - acc: 0.6500\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.85013\n",
      "Epoch 123/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.8800 - acc: 0.6000\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.85013\n",
      "Epoch 124/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.8661 - acc: 0.6000\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.85013\n",
      "Epoch 125/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.8493 - acc: 0.6000\n",
      "\n",
      "Epoch 00125: loss improved from 0.85013 to 0.84928, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 126/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.8431 - acc: 0.6500\n",
      "\n",
      "Epoch 00126: loss improved from 0.84928 to 0.84306, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 127/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.8168 - acc: 0.6000\n",
      "\n",
      "Epoch 00127: loss improved from 0.84306 to 0.81682, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 128/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.7977 - acc: 0.6500\n",
      "\n",
      "Epoch 00128: loss improved from 0.81682 to 0.79771, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 129/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.7879 - acc: 0.7000\n",
      "\n",
      "Epoch 00129: loss improved from 0.79771 to 0.78790, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 130/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.7593 - acc: 0.7000\n",
      "\n",
      "Epoch 00130: loss improved from 0.78790 to 0.75927, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 131/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.7394 - acc: 0.7000\n",
      "\n",
      "Epoch 00131: loss improved from 0.75927 to 0.73944, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 132/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.7372 - acc: 0.6500\n",
      "\n",
      "Epoch 00132: loss improved from 0.73944 to 0.73716, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 133/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.7243 - acc: 0.6500\n",
      "\n",
      "Epoch 00133: loss improved from 0.73716 to 0.72431, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 134/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.6947 - acc: 0.7000\n",
      "\n",
      "Epoch 00134: loss improved from 0.72431 to 0.69470, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 135/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.6916 - acc: 0.7000\n",
      "\n",
      "Epoch 00135: loss improved from 0.69470 to 0.69162, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 136/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.7106 - acc: 0.7000\n",
      "\n",
      "Epoch 00136: loss did not improve from 0.69162\n",
      "Epoch 137/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.6502 - acc: 0.7000\n",
      "\n",
      "Epoch 00137: loss improved from 0.69162 to 0.65025, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 138/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.6413 - acc: 0.7500\n",
      "\n",
      "Epoch 00138: loss improved from 0.65025 to 0.64131, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 139/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.6397 - acc: 0.7500\n",
      "\n",
      "Epoch 00139: loss improved from 0.64131 to 0.63968, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 140/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.6199 - acc: 0.7500\n",
      "\n",
      "Epoch 00140: loss improved from 0.63968 to 0.61993, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 141/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.6025 - acc: 0.7500\n",
      "\n",
      "Epoch 00141: loss improved from 0.61993 to 0.60252, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 142/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5921 - acc: 0.7500\n",
      "\n",
      "Epoch 00142: loss improved from 0.60252 to 0.59205, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 143/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5798 - acc: 0.7500\n",
      "\n",
      "Epoch 00143: loss improved from 0.59205 to 0.57982, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 144/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5678 - acc: 0.7500\n",
      "\n",
      "Epoch 00144: loss improved from 0.57982 to 0.56783, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5590 - acc: 0.7500\n",
      "\n",
      "Epoch 00145: loss improved from 0.56783 to 0.55900, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 146/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5540 - acc: 0.7500\n",
      "\n",
      "Epoch 00146: loss improved from 0.55900 to 0.55405, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 147/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5448 - acc: 0.7500\n",
      "\n",
      "Epoch 00147: loss improved from 0.55405 to 0.54482, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 148/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5304 - acc: 0.7500\n",
      "\n",
      "Epoch 00148: loss improved from 0.54482 to 0.53042, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 149/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5360 - acc: 0.7500\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.53042\n",
      "Epoch 150/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5223 - acc: 0.7500\n",
      "\n",
      "Epoch 00150: loss improved from 0.53042 to 0.52233, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 151/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.5120 - acc: 0.7500\n",
      "\n",
      "Epoch 00151: loss improved from 0.52233 to 0.51199, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 152/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5053 - acc: 0.7500\n",
      "\n",
      "Epoch 00152: loss improved from 0.51199 to 0.50527, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 153/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4983 - acc: 0.7500\n",
      "\n",
      "Epoch 00153: loss improved from 0.50527 to 0.49827, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 154/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4926 - acc: 0.7500\n",
      "\n",
      "Epoch 00154: loss improved from 0.49827 to 0.49262, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 155/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4892 - acc: 0.7500\n",
      "\n",
      "Epoch 00155: loss improved from 0.49262 to 0.48921, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 156/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.4845 - acc: 0.7500\n",
      "\n",
      "Epoch 00156: loss improved from 0.48921 to 0.48446, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 157/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4772 - acc: 0.7500\n",
      "\n",
      "Epoch 00157: loss improved from 0.48446 to 0.47723, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 158/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4725 - acc: 0.7500\n",
      "\n",
      "Epoch 00158: loss improved from 0.47723 to 0.47246, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 159/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4684 - acc: 0.8500\n",
      "\n",
      "Epoch 00159: loss improved from 0.47246 to 0.46837, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 160/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4614 - acc: 0.8000\n",
      "\n",
      "Epoch 00160: loss improved from 0.46837 to 0.46144, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 161/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.4563 - acc: 0.8000\n",
      "\n",
      "Epoch 00161: loss improved from 0.46144 to 0.45627, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 162/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4527 - acc: 0.8000\n",
      "\n",
      "Epoch 00162: loss improved from 0.45627 to 0.45266, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 163/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4481 - acc: 0.8000\n",
      "\n",
      "Epoch 00163: loss improved from 0.45266 to 0.44807, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 164/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4439 - acc: 0.8000\n",
      "\n",
      "Epoch 00164: loss improved from 0.44807 to 0.44393, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 165/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4398 - acc: 0.8000\n",
      "\n",
      "Epoch 00165: loss improved from 0.44393 to 0.43983, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 166/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4378 - acc: 0.8000\n",
      "\n",
      "Epoch 00166: loss improved from 0.43983 to 0.43777, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 167/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4353 - acc: 0.8000\n",
      "\n",
      "Epoch 00167: loss improved from 0.43777 to 0.43533, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 168/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4320 - acc: 0.8000\n",
      "\n",
      "Epoch 00168: loss improved from 0.43533 to 0.43197, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 169/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4239 - acc: 0.8000\n",
      "\n",
      "Epoch 00169: loss improved from 0.43197 to 0.42389, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 170/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4173 - acc: 0.8000\n",
      "\n",
      "Epoch 00170: loss improved from 0.42389 to 0.41735, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 171/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4165 - acc: 0.8000\n",
      "\n",
      "Epoch 00171: loss improved from 0.41735 to 0.41652, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 172/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4038 - acc: 0.8500\n",
      "\n",
      "Epoch 00172: loss improved from 0.41652 to 0.40382, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 173/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4033 - acc: 0.8500\n",
      "\n",
      "Epoch 00173: loss improved from 0.40382 to 0.40332, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 174/200\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.4003 - acc: 0.8500\n",
      "\n",
      "Epoch 00174: loss improved from 0.40332 to 0.40031, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 175/200\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.3897 - acc: 0.8500\n",
      "\n",
      "Epoch 00175: loss improved from 0.40031 to 0.38973, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 176/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.3819 - acc: 0.8500\n",
      "\n",
      "Epoch 00176: loss improved from 0.38973 to 0.38191, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 177/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3729 - acc: 0.8500\n",
      "\n",
      "Epoch 00177: loss improved from 0.38191 to 0.37290, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 178/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3694 - acc: 0.8500\n",
      "\n",
      "Epoch 00178: loss improved from 0.37290 to 0.36943, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 179/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3599 - acc: 0.8500\n",
      "\n",
      "Epoch 00179: loss improved from 0.36943 to 0.35986, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 180/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3528 - acc: 0.8500\n",
      "\n",
      "Epoch 00180: loss improved from 0.35986 to 0.35284, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 181/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.3467 - acc: 0.8500\n",
      "\n",
      "Epoch 00181: loss improved from 0.35284 to 0.34674, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 182/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3399 - acc: 0.8500\n",
      "\n",
      "Epoch 00182: loss improved from 0.34674 to 0.33987, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 183/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3388 - acc: 0.8500\n",
      "\n",
      "Epoch 00183: loss improved from 0.33987 to 0.33878, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 184/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3322 - acc: 0.8500\n",
      "\n",
      "Epoch 00184: loss improved from 0.33878 to 0.33215, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 185/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3263 - acc: 0.8500\n",
      "\n",
      "Epoch 00185: loss improved from 0.33215 to 0.32629, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3213 - acc: 0.8500\n",
      "\n",
      "Epoch 00186: loss improved from 0.32629 to 0.32133, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 187/200\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.3175 - acc: 0.8500\n",
      "\n",
      "Epoch 00187: loss improved from 0.32133 to 0.31751, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 188/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3092 - acc: 0.8500\n",
      "\n",
      "Epoch 00188: loss improved from 0.31751 to 0.30917, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 189/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3065 - acc: 0.8500\n",
      "\n",
      "Epoch 00189: loss improved from 0.30917 to 0.30654, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 190/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3035 - acc: 0.8500\n",
      "\n",
      "Epoch 00190: loss improved from 0.30654 to 0.30349, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 191/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.2998 - acc: 0.8500\n",
      "\n",
      "Epoch 00191: loss improved from 0.30349 to 0.29981, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 192/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.2932 - acc: 0.8500\n",
      "\n",
      "Epoch 00192: loss improved from 0.29981 to 0.29315, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 193/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.2915 - acc: 0.8500\n",
      "\n",
      "Epoch 00193: loss improved from 0.29315 to 0.29150, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 194/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.3094 - acc: 0.8500\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.29150\n",
      "Epoch 195/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.7211 - acc: 0.8000\n",
      "\n",
      "Epoch 00195: loss did not improve from 0.29150\n",
      "Epoch 196/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.5458 - acc: 0.6500\n",
      "\n",
      "Epoch 00196: loss did not improve from 0.29150\n",
      "Epoch 197/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 2.3287 - acc: 0.5500\n",
      "\n",
      "Epoch 00197: loss did not improve from 0.29150\n",
      "Epoch 198/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.2782 - acc: 0.7500\n",
      "\n",
      "Epoch 00198: loss did not improve from 0.29150\n",
      "Epoch 199/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 2.7741 - acc: 0.5000\n",
      "\n",
      "Epoch 00199: loss did not improve from 0.29150\n",
      "Epoch 200/200\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 1.1574 - acc: 0.8000\n",
      "\n",
      "Epoch 00200: loss did not improve from 0.29150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c7d5acaf28>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 16, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 23962)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 23962, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 23962, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 23962, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 23962, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 23962, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 23962, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 23962, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 23962, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 23962, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 23962, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 23962, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 23962, 40)    128040      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 300, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 300, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 300, 10)      40010       conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 10)        0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 10)        210         downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 10)        0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 10)           0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 10)           0           reshape_15[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 344,660\n",
      "Trainable params: 344,660\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # 2 samples for 1 user\n",
    "# no_users = 10\n",
    "# dilation_depth = 2\n",
    "\n",
    "wnc = WaveNetClassifier((23962,), (10,), kernel_size = 2, dilation_depth = 2, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20/20 [==============================] - 10s 509ms/step - loss: 2.3029 - acc: 0.0500\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.30293, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 2.3029 - acc: 0.1000\n",
      "\n",
      "Epoch 00002: loss improved from 2.30293 to 2.30290, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3036 - acc: 0.1000\n",
      "\n",
      "Epoch 00003: loss did not improve from 2.30290\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3037 - acc: 0.1000\n",
      "\n",
      "Epoch 00004: loss did not improve from 2.30290\n",
      "Epoch 5/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00005: loss improved from 2.30290 to 2.30261, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00006: loss did not improve from 2.30261\n",
      "Epoch 7/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3028 - acc: 0.1000\n",
      "\n",
      "Epoch 00007: loss did not improve from 2.30261\n",
      "Epoch 8/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3028 - acc: 0.1000\n",
      "\n",
      "Epoch 00008: loss did not improve from 2.30261\n",
      "Epoch 9/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00009: loss did not improve from 2.30261\n",
      "Epoch 10/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00010: loss did not improve from 2.30261\n",
      "Epoch 11/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00011: loss did not improve from 2.30261\n",
      "Epoch 12/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00012: loss improved from 2.30261 to 2.30256, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 13/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00013: loss did not improve from 2.30256\n",
      "Epoch 14/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00014: loss did not improve from 2.30256\n",
      "Epoch 15/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00015: loss did not improve from 2.30256\n",
      "Epoch 16/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.30256\n",
      "Epoch 17/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.1000\n",
      "\n",
      "Epoch 00017: loss did not improve from 2.30256\n",
      "Epoch 18/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3026 - acc: 0.0500\n",
      "\n",
      "Epoch 00018: loss did not improve from 2.30256\n",
      "Epoch 19/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3027 - acc: 0.1000\n",
      "\n",
      "Epoch 00019: loss did not improve from 2.30256\n",
      "Epoch 20/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3025 - acc: 0.1000\n",
      "\n",
      "Epoch 00020: loss improved from 2.30256 to 2.30253, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 21/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3025 - acc: 0.1000\n",
      "\n",
      "Epoch 00021: loss did not improve from 2.30253\n",
      "Epoch 22/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3025 - acc: 0.1000\n",
      "\n",
      "Epoch 00022: loss improved from 2.30253 to 2.30249, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 23/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3024 - acc: 0.1000\n",
      "\n",
      "Epoch 00023: loss improved from 2.30249 to 2.30238, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3020 - acc: 0.1000\n",
      "\n",
      "Epoch 00024: loss improved from 2.30238 to 2.30197, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 25/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3012 - acc: 0.1500\n",
      "\n",
      "Epoch 00025: loss improved from 2.30197 to 2.30123, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 26/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3007 - acc: 0.1000\n",
      "\n",
      "Epoch 00026: loss improved from 2.30123 to 2.30068, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 27/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3096 - acc: 0.1000\n",
      "\n",
      "Epoch 00027: loss did not improve from 2.30068\n",
      "Epoch 28/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.2950 - acc: 0.1000\n",
      "\n",
      "Epoch 00028: loss improved from 2.30068 to 2.29503, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 29/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3098 - acc: 0.1000\n",
      "\n",
      "Epoch 00029: loss did not improve from 2.29503\n",
      "Epoch 30/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.3037 - acc: 0.1000\n",
      "\n",
      "Epoch 00030: loss did not improve from 2.29503\n",
      "Epoch 31/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.2934 - acc: 0.1000\n",
      "\n",
      "Epoch 00031: loss improved from 2.29503 to 2.29344, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 32/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.2825 - acc: 0.1000\n",
      "\n",
      "Epoch 00032: loss improved from 2.29344 to 2.28246, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 33/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.2750 - acc: 0.1000\n",
      "\n",
      "Epoch 00033: loss improved from 2.28246 to 2.27497, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 34/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.2597 - acc: 0.1500\n",
      "\n",
      "Epoch 00034: loss improved from 2.27497 to 2.25967, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 35/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.2540 - acc: 0.1000\n",
      "\n",
      "Epoch 00035: loss improved from 2.25967 to 2.25399, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 36/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.2318 - acc: 0.1500\n",
      "\n",
      "Epoch 00036: loss improved from 2.25399 to 2.23179, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 37/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.2126 - acc: 0.1000\n",
      "\n",
      "Epoch 00037: loss improved from 2.23179 to 2.21258, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 38/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.1915 - acc: 0.1000\n",
      "\n",
      "Epoch 00038: loss improved from 2.21258 to 2.19149, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 39/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.1538 - acc: 0.1000\n",
      "\n",
      "Epoch 00039: loss improved from 2.19149 to 2.15384, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 40/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.1460 - acc: 0.1500\n",
      "\n",
      "Epoch 00040: loss improved from 2.15384 to 2.14600, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 41/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.1391 - acc: 0.1500\n",
      "\n",
      "Epoch 00041: loss improved from 2.14600 to 2.13914, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 42/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.1189 - acc: 0.2000\n",
      "\n",
      "Epoch 00042: loss improved from 2.13914 to 2.11893, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 43/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2.0729 - acc: 0.1500\n",
      "\n",
      "Epoch 00043: loss improved from 2.11893 to 2.07289, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 44/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.1209 - acc: 0.1000\n",
      "\n",
      "Epoch 00044: loss did not improve from 2.07289\n",
      "Epoch 45/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.0502 - acc: 0.1500\n",
      "\n",
      "Epoch 00045: loss improved from 2.07289 to 2.05018, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 46/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.0409 - acc: 0.2000\n",
      "\n",
      "Epoch 00046: loss improved from 2.05018 to 2.04093, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 47/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.0298 - acc: 0.2000\n",
      "\n",
      "Epoch 00047: loss improved from 2.04093 to 2.02985, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 2.4372 - acc: 0.0500\n",
      "\n",
      "Epoch 00048: loss did not improve from 2.02985\n",
      "Epoch 49/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.0322 - acc: 0.1500\n",
      "\n",
      "Epoch 00049: loss did not improve from 2.02985\n",
      "Epoch 50/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.9990 - acc: 0.1500\n",
      "\n",
      "Epoch 00050: loss improved from 2.02985 to 1.99898, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 51/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.0438 - acc: 0.1500\n",
      "\n",
      "Epoch 00051: loss did not improve from 1.99898\n",
      "Epoch 52/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.0188 - acc: 0.1500\n",
      "\n",
      "Epoch 00052: loss did not improve from 1.99898\n",
      "Epoch 53/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 2.0096 - acc: 0.2000\n",
      "\n",
      "Epoch 00053: loss did not improve from 1.99898\n",
      "Epoch 54/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 2.0036 - acc: 0.2000\n",
      "\n",
      "Epoch 00054: loss did not improve from 1.99898\n",
      "Epoch 55/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.9809 - acc: 0.2000\n",
      "\n",
      "Epoch 00055: loss improved from 1.99898 to 1.98092, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 56/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.9709 - acc: 0.1500\n",
      "\n",
      "Epoch 00056: loss improved from 1.98092 to 1.97091, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 57/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.9535 - acc: 0.1500\n",
      "\n",
      "Epoch 00057: loss improved from 1.97091 to 1.95352, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 58/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.9528 - acc: 0.2500\n",
      "\n",
      "Epoch 00058: loss improved from 1.95352 to 1.95282, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 59/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.9593 - acc: 0.1500\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.95282\n",
      "Epoch 60/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.9321 - acc: 0.2000\n",
      "\n",
      "Epoch 00060: loss improved from 1.95282 to 1.93210, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 61/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.9044 - acc: 0.2500\n",
      "\n",
      "Epoch 00061: loss improved from 1.93210 to 1.90438, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 62/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.9130 - acc: 0.1500\n",
      "\n",
      "Epoch 00062: loss did not improve from 1.90438\n",
      "Epoch 63/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.9044 - acc: 0.2000\n",
      "\n",
      "Epoch 00063: loss improved from 1.90438 to 1.90437, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 64/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.8833 - acc: 0.2500\n",
      "\n",
      "Epoch 00064: loss improved from 1.90437 to 1.88331, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 65/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.8615 - acc: 0.3000\n",
      "\n",
      "Epoch 00065: loss improved from 1.88331 to 1.86148, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 66/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.8553 - acc: 0.3000\n",
      "\n",
      "Epoch 00066: loss improved from 1.86148 to 1.85531, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 67/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.8231 - acc: 0.3500\n",
      "\n",
      "Epoch 00067: loss improved from 1.85531 to 1.82311, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 68/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.8012 - acc: 0.4000\n",
      "\n",
      "Epoch 00068: loss improved from 1.82311 to 1.80117, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 69/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.8064 - acc: 0.3500\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.80117\n",
      "Epoch 70/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.7584 - acc: 0.3500\n",
      "\n",
      "Epoch 00070: loss improved from 1.80117 to 1.75839, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 71/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.7367 - acc: 0.3000\n",
      "\n",
      "Epoch 00071: loss improved from 1.75839 to 1.73667, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 72/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.7304 - acc: 0.3000\n",
      "\n",
      "Epoch 00072: loss improved from 1.73667 to 1.73040, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 73/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.6978 - acc: 0.3000\n",
      "\n",
      "Epoch 00073: loss improved from 1.73040 to 1.69784, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 74/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.6640 - acc: 0.3500\n",
      "\n",
      "Epoch 00074: loss improved from 1.69784 to 1.66400, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 75/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.6460 - acc: 0.4000\n",
      "\n",
      "Epoch 00075: loss improved from 1.66400 to 1.64602, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 76/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.6020 - acc: 0.4000\n",
      "\n",
      "Epoch 00076: loss improved from 1.64602 to 1.60200, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 77/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.6231 - acc: 0.3500\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.60200\n",
      "Epoch 78/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.5546 - acc: 0.3500\n",
      "\n",
      "Epoch 00078: loss improved from 1.60200 to 1.55457, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 79/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.5343 - acc: 0.3500\n",
      "\n",
      "Epoch 00079: loss improved from 1.55457 to 1.53427, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 80/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.5299 - acc: 0.4000\n",
      "\n",
      "Epoch 00080: loss improved from 1.53427 to 1.52988, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 81/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.4871 - acc: 0.4000\n",
      "\n",
      "Epoch 00081: loss improved from 1.52988 to 1.48715, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 82/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.4259 - acc: 0.4000\n",
      "\n",
      "Epoch 00082: loss improved from 1.48715 to 1.42586, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 83/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.4670 - acc: 0.4500\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.42586\n",
      "Epoch 84/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.4505 - acc: 0.4000\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.42586\n",
      "Epoch 85/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.4367 - acc: 0.4000\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.42586\n",
      "Epoch 86/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.3616 - acc: 0.4500\n",
      "\n",
      "Epoch 00086: loss improved from 1.42586 to 1.36157, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 87/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.3494 - acc: 0.5500\n",
      "\n",
      "Epoch 00087: loss improved from 1.36157 to 1.34943, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 88/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.3516 - acc: 0.5000\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.34943\n",
      "Epoch 89/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.3241 - acc: 0.5000\n",
      "\n",
      "Epoch 00089: loss improved from 1.34943 to 1.32409, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 90/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.2849 - acc: 0.5500\n",
      "\n",
      "Epoch 00090: loss improved from 1.32409 to 1.28491, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 91/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.2171 - acc: 0.6000\n",
      "\n",
      "Epoch 00091: loss improved from 1.28491 to 1.21712, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 92/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.2040 - acc: 0.5500\n",
      "\n",
      "Epoch 00092: loss improved from 1.21712 to 1.20399, saving model to results3/saved_wavenet_clasifier.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/200\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 1.1494 - acc: 0.6000\n",
      "\n",
      "Epoch 00093: loss improved from 1.20399 to 1.14938, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 94/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.1544 - acc: 0.6000\n",
      "\n",
      "Epoch 00094: loss did not improve from 1.14938\n",
      "Epoch 95/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.1412 - acc: 0.6500\n",
      "\n",
      "Epoch 00095: loss improved from 1.14938 to 1.14125, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 96/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.1128 - acc: 0.5500\n",
      "\n",
      "Epoch 00096: loss improved from 1.14125 to 1.11281, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 97/200\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 1.1001 - acc: 0.4500\n",
      "\n",
      "Epoch 00097: loss improved from 1.11281 to 1.10008, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 98/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.3467 - acc: 0.3000\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.10008\n",
      "Epoch 99/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.0541 - acc: 0.6500\n",
      "\n",
      "Epoch 00099: loss improved from 1.10008 to 1.05410, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 100/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.3997 - acc: 0.4000\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.05410\n",
      "Epoch 101/200\n",
      "20/20 [==============================] - 1s 32ms/step - loss: 1.1408 - acc: 0.5500\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.05410\n",
      "Epoch 102/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.3100 - acc: 0.5000\n",
      "\n",
      "Epoch 00102: loss did not improve from 1.05410\n",
      "Epoch 103/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 1.2305 - acc: 0.5500\n",
      "\n",
      "Epoch 00103: loss did not improve from 1.05410\n",
      "Epoch 104/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.1360 - acc: 0.5000\n",
      "\n",
      "Epoch 00104: loss did not improve from 1.05410\n",
      "Epoch 105/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.0438 - acc: 0.5500\n",
      "\n",
      "Epoch 00105: loss improved from 1.05410 to 1.04380, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 106/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.0955 - acc: 0.6000\n",
      "\n",
      "Epoch 00106: loss did not improve from 1.04380\n",
      "Epoch 107/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 1.0294 - acc: 0.5500\n",
      "\n",
      "Epoch 00107: loss improved from 1.04380 to 1.02944, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 108/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.9600 - acc: 0.6500\n",
      "\n",
      "Epoch 00108: loss improved from 1.02944 to 0.96003, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 109/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.9635 - acc: 0.5500\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.96003\n",
      "Epoch 110/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.8659 - acc: 0.7000\n",
      "\n",
      "Epoch 00110: loss improved from 0.96003 to 0.86594, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 111/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.8259 - acc: 0.7500\n",
      "\n",
      "Epoch 00111: loss improved from 0.86594 to 0.82588, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 112/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.9073 - acc: 0.6500\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.82588\n",
      "Epoch 113/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.8838 - acc: 0.7000\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.82588\n",
      "Epoch 114/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.7727 - acc: 0.7000\n",
      "\n",
      "Epoch 00114: loss improved from 0.82588 to 0.77267, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 115/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.7665 - acc: 0.7000\n",
      "\n",
      "Epoch 00115: loss improved from 0.77267 to 0.76653, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 116/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.7890 - acc: 0.7000\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.76653\n",
      "Epoch 117/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.6942 - acc: 0.7500\n",
      "\n",
      "Epoch 00117: loss improved from 0.76653 to 0.69416, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 118/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.8533 - acc: 0.6500\n",
      "\n",
      "Epoch 00118: loss did not improve from 0.69416\n",
      "Epoch 119/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.7342 - acc: 0.7000\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.69416\n",
      "Epoch 120/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.6300 - acc: 0.8000\n",
      "\n",
      "Epoch 00120: loss improved from 0.69416 to 0.62998, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 121/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.6147 - acc: 0.7500\n",
      "\n",
      "Epoch 00121: loss improved from 0.62998 to 0.61472, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 122/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.5742 - acc: 0.8000\n",
      "\n",
      "Epoch 00122: loss improved from 0.61472 to 0.57424, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 123/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.5331 - acc: 0.8000\n",
      "\n",
      "Epoch 00123: loss improved from 0.57424 to 0.53310, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 124/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.5525 - acc: 0.8000\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.53310\n",
      "Epoch 125/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.5556 - acc: 0.8000\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.53310\n",
      "Epoch 126/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.5010 - acc: 0.8000\n",
      "\n",
      "Epoch 00126: loss improved from 0.53310 to 0.50103, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 127/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.4720 - acc: 0.8000\n",
      "\n",
      "Epoch 00127: loss improved from 0.50103 to 0.47196, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 128/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.4557 - acc: 0.9000\n",
      "\n",
      "Epoch 00128: loss improved from 0.47196 to 0.45567, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 129/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.4359 - acc: 0.9000\n",
      "\n",
      "Epoch 00129: loss improved from 0.45567 to 0.43595, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 130/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.4162 - acc: 0.8500\n",
      "\n",
      "Epoch 00130: loss improved from 0.43595 to 0.41616, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 131/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.3835 - acc: 0.8500\n",
      "\n",
      "Epoch 00131: loss improved from 0.41616 to 0.38352, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 132/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.3571 - acc: 0.8500\n",
      "\n",
      "Epoch 00132: loss improved from 0.38352 to 0.35708, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 133/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.3386 - acc: 0.8500\n",
      "\n",
      "Epoch 00133: loss improved from 0.35708 to 0.33859, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 134/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.3280 - acc: 0.9000\n",
      "\n",
      "Epoch 00134: loss improved from 0.33859 to 0.32801, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 135/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2926 - acc: 0.8500\n",
      "\n",
      "Epoch 00135: loss improved from 0.32801 to 0.29258, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 136/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.3331 - acc: 0.9000\n",
      "\n",
      "Epoch 00136: loss did not improve from 0.29258\n",
      "Epoch 137/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2972 - acc: 0.9000\n",
      "\n",
      "Epoch 00137: loss did not improve from 0.29258\n",
      "Epoch 138/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.3182 - acc: 0.8500\n",
      "\n",
      "Epoch 00138: loss did not improve from 0.29258\n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2648 - acc: 0.9000\n",
      "\n",
      "Epoch 00139: loss improved from 0.29258 to 0.26483, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 140/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.7555 - acc: 0.7500\n",
      "\n",
      "Epoch 00140: loss did not improve from 0.26483\n",
      "Epoch 141/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 0.2649 - acc: 0.8500\n",
      "\n",
      "Epoch 00141: loss did not improve from 0.26483\n",
      "Epoch 142/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.5734 - acc: 0.7500\n",
      "\n",
      "Epoch 00142: loss did not improve from 0.26483\n",
      "Epoch 143/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.4622 - acc: 0.8000\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.26483\n",
      "Epoch 144/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2712 - acc: 0.9000\n",
      "\n",
      "Epoch 00144: loss did not improve from 0.26483\n",
      "Epoch 145/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2706 - acc: 0.9000\n",
      "\n",
      "Epoch 00145: loss did not improve from 0.26483\n",
      "Epoch 146/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.3147 - acc: 0.9000\n",
      "\n",
      "Epoch 00146: loss did not improve from 0.26483\n",
      "Epoch 147/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.3044 - acc: 0.9000\n",
      "\n",
      "Epoch 00147: loss did not improve from 0.26483\n",
      "Epoch 148/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2631 - acc: 0.9000\n",
      "\n",
      "Epoch 00148: loss improved from 0.26483 to 0.26307, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 149/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2526 - acc: 0.8500\n",
      "\n",
      "Epoch 00149: loss improved from 0.26307 to 0.25263, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 150/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2509 - acc: 0.8500\n",
      "\n",
      "Epoch 00150: loss improved from 0.25263 to 0.25091, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 151/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2476 - acc: 0.8500\n",
      "\n",
      "Epoch 00151: loss improved from 0.25091 to 0.24760, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 152/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2288 - acc: 0.9500\n",
      "\n",
      "Epoch 00152: loss improved from 0.24760 to 0.22876, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 153/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2032 - acc: 0.9500\n",
      "\n",
      "Epoch 00153: loss improved from 0.22876 to 0.20317, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 154/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2063 - acc: 0.9000\n",
      "\n",
      "Epoch 00154: loss did not improve from 0.20317\n",
      "Epoch 155/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2402 - acc: 0.9000\n",
      "\n",
      "Epoch 00155: loss did not improve from 0.20317\n",
      "Epoch 156/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2264 - acc: 0.9000\n",
      "\n",
      "Epoch 00156: loss did not improve from 0.20317\n",
      "Epoch 157/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1915 - acc: 0.9000\n",
      "\n",
      "Epoch 00157: loss improved from 0.20317 to 0.19146, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 158/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1637 - acc: 1.0000\n",
      "\n",
      "Epoch 00158: loss improved from 0.19146 to 0.16374, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 159/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1553 - acc: 1.0000\n",
      "\n",
      "Epoch 00159: loss improved from 0.16374 to 0.15529, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 160/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 0.1542 - acc: 0.9500\n",
      "\n",
      "Epoch 00160: loss improved from 0.15529 to 0.15425, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 161/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 0.1571 - acc: 0.9500\n",
      "\n",
      "Epoch 00161: loss did not improve from 0.15425\n",
      "Epoch 162/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1448 - acc: 0.9500\n",
      "\n",
      "Epoch 00162: loss improved from 0.15425 to 0.14478, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 163/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1337 - acc: 1.0000\n",
      "\n",
      "Epoch 00163: loss improved from 0.14478 to 0.13372, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 164/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1244 - acc: 1.0000\n",
      "\n",
      "Epoch 00164: loss improved from 0.13372 to 0.12443, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 165/200\n",
      "20/20 [==============================] - 1s 31ms/step - loss: 0.1202 - acc: 1.0000\n",
      "\n",
      "Epoch 00165: loss improved from 0.12443 to 0.12021, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 166/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1227 - acc: 0.9500\n",
      "\n",
      "Epoch 00166: loss did not improve from 0.12021\n",
      "Epoch 167/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1222 - acc: 0.9500\n",
      "\n",
      "Epoch 00167: loss did not improve from 0.12021\n",
      "Epoch 168/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.1120 - acc: 0.9500\n",
      "\n",
      "Epoch 00168: loss improved from 0.12021 to 0.11202, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 169/200\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.0949 - acc: 1.0000\n",
      "\n",
      "Epoch 00169: loss improved from 0.11202 to 0.09489, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-204-d92e074264e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 16, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 18330)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 18330, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 18330, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 18330, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 18330, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 18330, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 18330, 40)    128040      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 230, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 230, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 230, 100)     400100      conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 100)       0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 100)       20100       downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 100)       0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100)          0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 100)          0           reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 724,640\n",
      "Trainable params: 724,640\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # 2 samples for 1 user\n",
    "# no_users = 400\n",
    "# dilation_depth = 2\n",
    "\n",
    "wnc = WaveNetClassifier((18330,), (100,), kernel_size = 2, dilation_depth = 2, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "200/200 [==============================] - 15s 76ms/step - loss: 4.6122 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.61221, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.6057 - acc: 0.0100\n",
      "\n",
      "Epoch 00002: loss improved from 4.61221 to 4.60571, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.6058 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00003: loss did not improve from 4.60571\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.6057 - acc: 0.0100\n",
      "\n",
      "Epoch 00004: loss improved from 4.60571 to 4.60568, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.6057 - acc: 0.0000e+00\n",
      "\n",
      "Epoch 00005: loss improved from 4.60568 to 4.60566, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.6058 - acc: 0.0100\n",
      "\n",
      "Epoch 00006: loss did not improve from 4.60566\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 4.6052 - acc: 0.0150\n",
      "\n",
      "Epoch 00007: loss improved from 4.60566 to 4.60516, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.5949 - acc: 0.0050\n",
      "\n",
      "Epoch 00008: loss improved from 4.60516 to 4.59495, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.5115 - acc: 0.0100\n",
      "\n",
      "Epoch 00009: loss improved from 4.59495 to 4.51154, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.4664 - acc: 0.0150\n",
      "\n",
      "Epoch 00010: loss improved from 4.51154 to 4.46643, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.4112 - acc: 0.0200\n",
      "\n",
      "Epoch 00011: loss improved from 4.46643 to 4.41119, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.3688 - acc: 0.0250\n",
      "\n",
      "Epoch 00012: loss improved from 4.41119 to 4.36878, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.2880 - acc: 0.0250\n",
      "\n",
      "Epoch 00013: loss improved from 4.36878 to 4.28800, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.2199 - acc: 0.0300\n",
      "\n",
      "Epoch 00014: loss improved from 4.28800 to 4.21987, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.1892 - acc: 0.0350\n",
      "\n",
      "Epoch 00015: loss improved from 4.21987 to 4.18922, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.1073 - acc: 0.0450\n",
      "\n",
      "Epoch 00016: loss improved from 4.18922 to 4.10732, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 4.0994 - acc: 0.0550\n",
      "\n",
      "Epoch 00017: loss improved from 4.10732 to 4.09937, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 4.0712 - acc: 0.0700\n",
      "\n",
      "Epoch 00018: loss improved from 4.09937 to 4.07123, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 3.9131 - acc: 0.0850\n",
      "\n",
      "Epoch 00019: loss improved from 4.07123 to 3.91315, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 3.8077 - acc: 0.1000\n",
      "\n",
      "Epoch 00020: loss improved from 3.91315 to 3.80775, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 21/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 3.6818 - acc: 0.1200\n",
      "\n",
      "Epoch 00021: loss improved from 3.80775 to 3.68175, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 3.5540 - acc: 0.1300\n",
      "\n",
      "Epoch 00022: loss improved from 3.68175 to 3.55402, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 3.4752 - acc: 0.1550\n",
      "\n",
      "Epoch 00023: loss improved from 3.55402 to 3.47522, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 24/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 3.2691 - acc: 0.1550\n",
      "\n",
      "Epoch 00024: loss improved from 3.47522 to 3.26911, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 25/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 3.1267 - acc: 0.1900\n",
      "\n",
      "Epoch 00025: loss improved from 3.26911 to 3.12673, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 3.0874 - acc: 0.2100\n",
      "\n",
      "Epoch 00026: loss improved from 3.12673 to 3.08736, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 27/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 3.1525 - acc: 0.1850\n",
      "\n",
      "Epoch 00027: loss did not improve from 3.08736\n",
      "Epoch 28/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.9836 - acc: 0.2400\n",
      "\n",
      "Epoch 00028: loss improved from 3.08736 to 2.98357, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 29/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.7174 - acc: 0.3000\n",
      "\n",
      "Epoch 00029: loss improved from 2.98357 to 2.71736, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 30/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.5950 - acc: 0.3100\n",
      "\n",
      "Epoch 00030: loss improved from 2.71736 to 2.59503, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 31/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.4606 - acc: 0.3250\n",
      "\n",
      "Epoch 00031: loss improved from 2.59503 to 2.46064, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 32/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.2699 - acc: 0.3850\n",
      "\n",
      "Epoch 00032: loss improved from 2.46064 to 2.26994, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 33/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.3059 - acc: 0.3650\n",
      "\n",
      "Epoch 00033: loss did not improve from 2.26994\n",
      "Epoch 34/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.2001 - acc: 0.4200\n",
      "\n",
      "Epoch 00034: loss improved from 2.26994 to 2.20005, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 35/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.2650 - acc: 0.3800\n",
      "\n",
      "Epoch 00035: loss did not improve from 2.20005\n",
      "Epoch 36/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.3109 - acc: 0.3800\n",
      "\n",
      "Epoch 00036: loss did not improve from 2.20005\n",
      "Epoch 37/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 2.3049 - acc: 0.3850\n",
      "\n",
      "Epoch 00037: loss did not improve from 2.20005\n",
      "Epoch 38/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.9659 - acc: 0.4650\n",
      "\n",
      "Epoch 00038: loss improved from 2.20005 to 1.96586, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 39/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.8320 - acc: 0.5150\n",
      "\n",
      "Epoch 00039: loss improved from 1.96586 to 1.83196, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 40/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.7457 - acc: 0.5050\n",
      "\n",
      "Epoch 00040: loss improved from 1.83196 to 1.74566, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 41/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.9149 - acc: 0.4800\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.74566\n",
      "Epoch 42/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.9615 - acc: 0.4700\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.74566\n",
      "Epoch 43/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.5609 - acc: 0.5650\n",
      "\n",
      "Epoch 00043: loss improved from 1.74566 to 1.56090, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 44/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 5s 23ms/step - loss: 1.4781 - acc: 0.6150\n",
      "\n",
      "Epoch 00044: loss improved from 1.56090 to 1.47809, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 45/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.3900 - acc: 0.6200\n",
      "\n",
      "Epoch 00045: loss improved from 1.47809 to 1.38995, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 46/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.4253 - acc: 0.6000\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.38995\n",
      "Epoch 47/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.4739 - acc: 0.6150\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.38995\n",
      "Epoch 48/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.2394 - acc: 0.6350\n",
      "\n",
      "Epoch 00048: loss improved from 1.38995 to 1.23945, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 49/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.1756 - acc: 0.6400\n",
      "\n",
      "Epoch 00049: loss improved from 1.23945 to 1.17556, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 50/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.0237 - acc: 0.7450\n",
      "\n",
      "Epoch 00050: loss improved from 1.17556 to 1.02372, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 51/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.9286 - acc: 0.7300\n",
      "\n",
      "Epoch 00051: loss improved from 1.02372 to 0.92863, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 52/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.9248 - acc: 0.7300\n",
      "\n",
      "Epoch 00052: loss improved from 0.92863 to 0.92477, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 53/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.8903 - acc: 0.7600\n",
      "\n",
      "Epoch 00053: loss improved from 0.92477 to 0.89030, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 54/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.9665 - acc: 0.7400\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.89030\n",
      "Epoch 55/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 1.1992 - acc: 0.7150\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.89030\n",
      "Epoch 56/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 1.1123 - acc: 0.7500\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.89030\n",
      "Epoch 57/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 1.0826 - acc: 0.7500\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.89030\n",
      "Epoch 58/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 1.3316 - acc: 0.6950\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.89030\n",
      "Epoch 59/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 1.5020 - acc: 0.6650\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.89030\n",
      "Epoch 60/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 1.1391 - acc: 0.7350\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.89030\n",
      "Epoch 61/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.9334 - acc: 0.7800\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.89030\n",
      "Epoch 62/200\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.6838 - acc: 0.8300\n",
      "\n",
      "Epoch 00062: loss improved from 0.89030 to 0.68377, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 63/200\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.5543 - acc: 0.8650\n",
      "\n",
      "Epoch 00063: loss improved from 0.68377 to 0.55429, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 64/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.4326 - acc: 0.9050\n",
      "\n",
      "Epoch 00064: loss improved from 0.55429 to 0.43255, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 65/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.3761 - acc: 0.9100\n",
      "\n",
      "Epoch 00065: loss improved from 0.43255 to 0.37611, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 66/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.3818 - acc: 0.8950\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.37611\n",
      "Epoch 67/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.3354 - acc: 0.9150\n",
      "\n",
      "Epoch 00067: loss improved from 0.37611 to 0.33539, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 68/200\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.3120 - acc: 0.9150\n",
      "\n",
      "Epoch 00068: loss improved from 0.33539 to 0.31196, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 69/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.2742 - acc: 0.9100\n",
      "\n",
      "Epoch 00069: loss improved from 0.31196 to 0.27415, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 70/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.2534 - acc: 0.9300\n",
      "\n",
      "Epoch 00070: loss improved from 0.27415 to 0.25344, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 71/200\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.2560 - acc: 0.9150\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.25344\n",
      "Epoch 72/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.2356 - acc: 0.9200\n",
      "\n",
      "Epoch 00072: loss improved from 0.25344 to 0.23560, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 73/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.2146 - acc: 0.9300\n",
      "\n",
      "Epoch 00073: loss improved from 0.23560 to 0.21461, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 74/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.2094 - acc: 0.9350\n",
      "\n",
      "Epoch 00074: loss improved from 0.21461 to 0.20936, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 75/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.3119 - acc: 0.9150\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.20936\n",
      "Epoch 76/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.3594 - acc: 0.8850\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.20936\n",
      "Epoch 77/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.4055 - acc: 0.8800\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.20936\n",
      "Epoch 78/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.4998 - acc: 0.8800\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.20936\n",
      "Epoch 79/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.3151 - acc: 0.9100\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.20936\n",
      "Epoch 80/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.2435 - acc: 0.9150\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.20936\n",
      "Epoch 81/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.2311 - acc: 0.9350\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.20936\n",
      "Epoch 82/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.1994 - acc: 0.9400\n",
      "\n",
      "Epoch 00082: loss improved from 0.20936 to 0.19944, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 83/200\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.1576 - acc: 0.9600\n",
      "\n",
      "Epoch 00083: loss improved from 0.19944 to 0.15764, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 84/200\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.1453 - acc: 0.958 - 5s 23ms/step - loss: 0.1397 - acc: 0.9600\n",
      "\n",
      "Epoch 00084: loss improved from 0.15764 to 0.13968, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 85/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.1343 - acc: 0.9600\n",
      "\n",
      "Epoch 00085: loss improved from 0.13968 to 0.13433, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 86/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.1211 - acc: 0.9600\n",
      "\n",
      "Epoch 00086: loss improved from 0.13433 to 0.12109, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 87/200\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.1330 - acc: 0.9500\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.12109\n",
      "Epoch 88/200\n",
      " 64/200 [========>.....................] - ETA: 2s - loss: 0.1279 - acc: 0.9844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d92e074264e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 16, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 18330)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshaped_input (Reshape)        (None, 18330, 1)     0           original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 18330, 40)    120         reshaped_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 18330, 40)    3240        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 18330, 40)    0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 18330, 40)    3240        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 18330, 40)    0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 18330, 40)    1640        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 18330, 40)    0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 18330, 40)    0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 18330, 40)    128040      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 230, 40)      0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 230, 40)      160040      downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 230, 100)     400100      conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 3, 100)       0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 3, 100)       20100       downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (AveragePooling1D (None, 1, 100)       0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 100)          0           final_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           reshape_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 724,640\n",
      "Trainable params: 724,640\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # 9 samples for 1 user\n",
    "# no_users = 100\n",
    "# dilation_depth = 2\n",
    "\n",
    "wnc = WaveNetClassifier((18330,), (100,), kernel_size = 2, dilation_depth = 2, n_filters = 40, task = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "900/900 [==============================] - 22s 24ms/step - loss: 4.6080 - acc: 0.0044\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.60805, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 2/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0033\n",
      "\n",
      "Epoch 00002: loss improved from 4.60805 to 4.60614, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 3/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6060 - acc: 0.0067\n",
      "\n",
      "Epoch 00003: loss improved from 4.60614 to 4.60601, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 4/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0033\n",
      "\n",
      "Epoch 00004: loss did not improve from 4.60601\n",
      "Epoch 5/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6060 - acc: 0.0044\n",
      "\n",
      "Epoch 00005: loss improved from 4.60601 to 4.60597, saving model to results3/saved_wavenet_clasifier.h5\n",
      "Epoch 6/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0044\n",
      "\n",
      "Epoch 00006: loss did not improve from 4.60597\n",
      "Epoch 7/200\n",
      "900/900 [==============================] - 19s 22ms/step - loss: 4.6061 - acc: 0.0067\n",
      "\n",
      "Epoch 00007: loss did not improve from 4.60597\n",
      "Epoch 8/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0089\n",
      "\n",
      "Epoch 00008: loss did not improve from 4.60597\n",
      "Epoch 9/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6062 - acc: 0.0100\n",
      "\n",
      "Epoch 00009: loss did not improve from 4.60597\n",
      "Epoch 10/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6060 - acc: 0.0078\n",
      "\n",
      "Epoch 00010: loss did not improve from 4.60597\n",
      "Epoch 11/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6061 - acc: 0.0078\n",
      "\n",
      "Epoch 00011: loss did not improve from 4.60597\n",
      "Epoch 12/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00012: loss did not improve from 4.60597\n",
      "Epoch 13/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6061 - acc: 0.0056\n",
      "\n",
      "Epoch 00013: loss did not improve from 4.60597\n",
      "Epoch 14/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0067\n",
      "\n",
      "Epoch 00014: loss did not improve from 4.60597\n",
      "Epoch 15/200\n",
      "900/900 [==============================] - 19s 22ms/step - loss: 4.6061 - acc: 0.0044\n",
      "\n",
      "Epoch 00015: loss did not improve from 4.60597\n",
      "Epoch 16/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0067\n",
      "\n",
      "Epoch 00016: loss did not improve from 4.60597\n",
      "Epoch 17/200\n",
      "900/900 [==============================] - 19s 22ms/step - loss: 4.6061 - acc: 0.0089\n",
      "\n",
      "Epoch 00017: loss did not improve from 4.60597\n",
      "Epoch 18/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00018: loss did not improve from 4.60597\n",
      "Epoch 19/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00019: loss did not improve from 4.60597\n",
      "Epoch 20/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00020: loss did not improve from 4.60597\n",
      "Epoch 21/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0056\n",
      "\n",
      "Epoch 00021: loss did not improve from 4.60597\n",
      "Epoch 22/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6062 - acc: 0.0067\n",
      "\n",
      "Epoch 00022: loss did not improve from 4.60597\n",
      "Epoch 23/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00023: loss did not improve from 4.60597\n",
      "Epoch 24/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00024: loss did not improve from 4.60597\n",
      "Epoch 25/200\n",
      "900/900 [==============================] - 19s 22ms/step - loss: 4.6062 - acc: 0.0100\n",
      "\n",
      "Epoch 00025: loss did not improve from 4.60597\n",
      "Epoch 26/200\n",
      "900/900 [==============================] - 19s 22ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00026: loss did not improve from 4.60597\n",
      "Epoch 27/200\n",
      "900/900 [==============================] - 19s 22ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00027: loss did not improve from 4.60597\n",
      "Epoch 28/200\n",
      "900/900 [==============================] - 19s 22ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00028: loss did not improve from 4.60597\n",
      "Epoch 29/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00029: loss did not improve from 4.60597\n",
      "Epoch 30/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00030: loss did not improve from 4.60597\n",
      "Epoch 31/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00031: loss did not improve from 4.60597\n",
      "Epoch 32/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00032: loss did not improve from 4.60597\n",
      "Epoch 33/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00033: loss did not improve from 4.60597\n",
      "Epoch 34/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00034: loss did not improve from 4.60597\n",
      "Epoch 35/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00035: loss did not improve from 4.60597\n",
      "Epoch 36/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00036: loss did not improve from 4.60597\n",
      "Epoch 37/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00037: loss did not improve from 4.60597\n",
      "Epoch 38/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00038: loss did not improve from 4.60597\n",
      "Epoch 39/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00039: loss did not improve from 4.60597\n",
      "Epoch 40/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6060 - acc: 0.0100\n",
      "\n",
      "Epoch 00040: loss did not improve from 4.60597\n",
      "Epoch 41/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6060 - acc: 0.0111\n",
      "\n",
      "Epoch 00041: loss did not improve from 4.60597\n",
      "Epoch 42/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00042: loss did not improve from 4.60597\n",
      "Epoch 43/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6061 - acc: 0.0100\n",
      "\n",
      "Epoch 00043: loss did not improve from 4.60597\n",
      "Epoch 44/200\n",
      "900/900 [==============================] - 20s 23ms/step - loss: 4.6061 - acc: 0.0067\n",
      "\n",
      "Epoch 00044: loss did not improve from 4.60597\n",
      "Epoch 45/200\n",
      "900/900 [==============================] - 20s 22ms/step - loss: 4.6061 - acc: 0.0089\n",
      "\n",
      "Epoch 00045: loss did not improve from 4.60597\n",
      "Epoch 46/200\n",
      "736/900 [=======================>......] - ETA: 3s - loss: 4.6058 - acc: 0.0095"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d92e074264e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwnc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results3/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\biometric-user-recognition\\wavenet-classifier\\WaveNetClassifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_data, epochs, batch_size, optimizer, save, save_dir)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wnc.fit(X_train, Y_train, epochs = 200, batch_size = 16, optimizer='adam', save=True, save_dir='results3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
